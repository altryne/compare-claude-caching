[00:00:00] 


## [00:00:00] Introduction and Celebrations

[00:00:00] **Alex Volkov:** Woo! Welcome everyone to ThursdAI, a celebratory July 4th edition, 4th of July edition. For some reason, Americans say July 4th for all of other dates, but 4th of July for this one. so happy 4th of July, happy Independence Day to everyone who's celebrating around the world. definitely a big one.

[00:00:51] **Alex Volkov:** Here, and obviously we're not taking a break because why would we? AI still happens, but also there's another celebration happening this week. 


## [00:00:59] The Journey of ThursdAI

[00:00:59] **Alex Volkov:** This week is the 52nd consecutive week that I've been publishing ThursdAI on Substack as a podcast. So those of you who follow along know that we started this as just conversations on X, live on X, and those were not recorded.

[00:01:15] **Alex Volkov:** and then for a while, people said, Hey, Alex, I cannot make it. Live. I'm driving or I have meetings, people are busy on Thursdays and they said, why don't you make this a podcast? this happens to other friends of ours who also do recording live on X. So shout out to other friends of us who do this now, the paper club with, Tanishq and Aran.

[00:01:34] **Alex Volkov:** they're getting the same feedback as well. And, I started ThursdAI as a podcast due to this feedback. And many of you specifically liked ThursdAI. This version, like listening to ThursdAI while driving, like listening to it, I don't know, later. this then turned into the whole thing, which turned into basically a new career for me, with Weights Biases.

[00:01:53] **Alex Volkov:** And this is incredible. And today marks the exact one year, Or I guess, exact 52 weeks since then, that we haven't taken one break and published, a hundred,throughout sicknesses, throughout vacation times. And so it's a very, big celebratory one for me. And for that, I'm very grateful that I have many friends and experts, joining me on stage here, for which, yeah, very grateful and very celebratory episode.

[00:02:17] **Alex Volkov:** And the one thing I will say is that next week I will actually take my first break from ThursdAI. I'll probably publish something, I'll probably publish like at least the newsletter will still go out with some links, but I think there will not be a live show next week as I'm going on vacation with my kids.

[00:02:34] **Alex Volkov:** But this week we had to talk about a bunch of stuff, because there's a bunch of stuff to cover. 


## [00:02:37] Special Guests and Global Celebrations

[00:02:37] **Alex Volkov:** before that, I want to say hi to a few friends of mine on stage. Nisten, first of all you, because you've joined most of these. I think you missed maybe two? Hey Nisten, how are you man?

[00:02:50] **Nisten Tahiraj:** Yeah, I'm good. I can hear the celebratory gunshots from across the border from my neighbors.

[00:03:00] **Alex Volkov:** Niton north of the border in Canada for those who are not familiar with the lore. and they have celebrated their independence previously already. Canada Day

[00:03:09] **Nisten Tahiraj:** yeah,

[00:03:10] **Alex Volkov:** it already happened. Yeah. And also we have, Wolfram joining us from Germany, I believe.

[00:03:19] **Wolfram Ravenwolf:** Yeah, exactly. So I didn't hear the fireworks yet, except for the ones you played, but happy 4th of July anyway. Happy anniversary.

[00:03:27] **Alex Volkov:** Thank you, man. Thank you. I appreciate you. And we also have Mazzy. Mazzy, I don't actually know where you're joining from. but, maybe tell us.

[00:03:34] **Maziyar Panahi:** Hi Alex, thanks for having on the show. Always a pleasure. I'm in France actually, so we're gonna have our own celebration in 10 days, 14th of July, pretty much the

[00:03:44] **Alex Volkov:** Bastile day, right?

[00:03:46] **Maziyar Panahi:** yeah, that's today. It's gonna be a big, celebration. A beautiful day, especially in Paris. and congratulations on, hosting this show for the community for the past year.

[00:03:54] **Maziyar Panahi:** it's not easy and it's very fruitful. Thank you so 

[00:03:56] **Maziyar Panahi:** much.

[00:03:57] **Alex Volkov:** Oh, I hear the most iconic French sound in your background. the ambulance, the iconic French ambulance sound. That's awesome.

[00:04:03] **Maziyar Panahi:** I actually live between the hospital and a police station, so you're going to hear a lot of them.

[00:04:09] **Alex Volkov:** and then I also want to welcome two new, friends, never been on the pod before. and, we'll have a deeper conversation with them after we start.

[00:04:17] **Alex Volkov:** So first I want to say hi to Lucas Atkins. Hey, Lucas, how are you? Nice to see you here.

[00:04:21] **Lucas Atkins:** Hey, I'm good. Congrats on a year, man. That's a big deal. while you're not in Texas right now, I am. And there was a the biggest firework I've ever heard went off right outside my apartment complex last night. And all the cars went crazy, so yeah, 

[00:04:37] **Lucas Atkins:** we're in the thick of it here.

[00:04:38] **Alex Volkov:** Yeah. Looking forward for tonight. Probably going to be a lot more. And, last but not least, definitely not least, a new friend I made last week at AI Engineer, Emil. Welcome to the show.

[00:04:50] **Emil Eifrem:** Hey, Alex. Great to be here joining you from a hotel room in Mallorca. Yeah, so hopefully the hotel Wi Fi is delivering.

[00:04:58] **Alex Volkov:** Yeah, coming through loud and clear, Emil. we'll do introductions later. but, everybody welcome. 


## [00:05:02] ThursdAI's Mission and Format

[00:05:02] **Alex Volkov:** I think it's time for us to do the TLDR. So folks who are joining, just now for the show, this is ThursdAI, your weekly AI news. Our motto is we stay up to date so you don't have to record live on X, as I said, every week for the past year plus.

[00:05:17] **Alex Volkov:** And this is the celebratory one year of the podcast edition. So everything we record in live show is also then edited and then published on the same day. So if you are not live here with us for the whole show and you need to step out, which is understandable, you can follow up on everything right here.

[00:05:33] **Alex Volkov:** On the podcatcher of your choice, Spotify, Apple, what have you. My name is Alex Volkov, I'm an AI evangelist with Weights Biases. Weights Biases is the reason this show continues and gets brought to you continuously. Because I work there and ThursdAI is part of my work at Weights Biases as well.

[00:05:51] **Alex Volkov:** So you'll hear some Weights Biases updates here during the show, obviously. But also, ThursdAI is a newsletter because many people don't like to listen for two hours of us talking about different things. They want the links and they want to just read. And so I also write a newsletter that people prefer that.

[00:06:08] **Alex Volkov:** Some people only get the newsletter. Some people only get the podcast. Some people do both depending on their needs. Their availability. that's great. That's great for me also to discover. last week I talked about this, but last week I met a lot of you listening to this and just gave a lot of high fives.

[00:06:23] **Alex Volkov:** It was incredible to me. Incredible standing on stage, talking to people, asking how many people listen to ThursdAI and seeing just Many hands go up, it was just absolutely incredible to me, and I'm really humbled and honored to be able to keep doing this weekly . And shout out to everybody who's listening, and for those of you who are new, welcome, welcome to the ThursdAI community.

[00:06:44] **Alex Volkov:** It's really fun to see how it grows, and how many incredible fine tuning and AI people, like AI engineers, are participating in this. I will just say this one thing, and we now have two people of this on stage. The one of the coolest things I get to do is to talk with people who made the news that week.

[00:07:02] **Alex Volkov:** And for this, we have Lucas here. We're going to talk about some cool things that they released, with RC, and then we're going to also dive deep into, Grafrag, which is a new concept to me. And Emil is going to help us understand everything about Grafrag. With that, the thing that we usually do for people who don't have the patience to sit with us for the full two hours is we start with a TLDR with everything that we're going to talk about on the show this week.


## [00:07:26] TLDR start

[00:07:26] **Alex Volkov:** So here's the TLDR. 

[00:07:28] **Alex Volkov:** here is the TLDR, everything we've talked about on ThursdAI, July 4th, 2024, 4th of July, a k, a independence day. Also AKA. Another reason to celebrate is this is the 52nd consecutive week. ThursdAI is published as a podcast, so it's been exactly. 52 weeks, a year. so hooray. and, I was very, very honored and excited to have multiple friends here who stuck around and been here also for a year.

[00:08:04] **Bartowski:** and we've talked about a lot of stuff today, and also we've had multiple people who actually built some of this stuff as well, joining us. So that was great.let's start with some of the updates. We started with Microsoft quietly updated PHI 3 [00:08:20] mini, PHI 3 in place, the mini version, both 4K and 128K. And the difference in performance is actually quite shocking to the point where I still think they really did themselves a disservice to not rename it something different. 

[00:08:33] **Alex Volkov:** So the reasoning capabilities have been updated, the math updated, very interesting to hear that they still, stealth updated it, even though it's basically the same model. 

[00:08:43] **Alex Volkov:** We also talked about InternLM, which is now the best open source model under 12 billion parameters on Hugging Face. it's. It beats state of the art math reasoning, surpassing models like LLama-3 and Gemma 2 9 billion parameters.

[00:08:58] **Alex Volkov:** In addition to that, it has a whopping 1 million context window. So they released a base version, a chat version, and a chat 1 million context window version. we also commented that InternLM released a bunch of tools. 

[00:09:12] **Alex Volkov:** We then talked about a new release from Arcee AI with Lucas Atkins, who works at Arcee.

[00:09:22] **Alex Volkov:** He's a research researcher there. Arcee, agent seven B. it's a initialized from Qwen two, Qwen two from our friends, at Alibaba. and it's focused on function and tool use specifically. we had a little bit of a deep dive into why this matters. And Arcee Agent is a 7 billion parameter language model 

[00:09:39] **Alex Volkov:** Excels at interpreting, executing, and chaining function calls, multiple function calls as well. So we had a chat with Lucas about why this model specifically gets, I think, number four on the Berkeley function calling benchmark called, Gorilla. While Lucas was here, he also participated in another open source release.

[00:09:58] **Alex Volkov:** And so as the tradition goes, we also had, folks who were working on another thing, cognitive computations, Eric Hartford, we talked about Dolphin Vision 72B, which shout out to Stable Quan, also friend of the pod, and some other folks, Fernando Neto and Lucas Atkin who was also here, talked about, Dolphin Vision 72B.

[00:10:18] **Alex Volkov:** the fun thing there is, first of all, it's also Qwen two There was a lot of celebration about Qwentu being the bell of the ball for finetuners right now, because it's incredible.Dolphin Vision 702B is the biggest open source vision language model that we currently have access to, it looks like.

[00:10:35] **Alex Volkov:** it's also unleashed and unfiltered, and Eric was talking about, that side of Dolphin specifically. and, this one was trained on a data set called sex, drugs, and rock and roll, from Fernando Neto,

[00:10:47] **Alex Volkov:** So if you want to try this out, Dolphin Vision is on Hug Face as well. And so we had Eric and, and Lucas also talk about this. 

[00:10:54] **Eric Hartford:** Fernando, in particular, he created a dataset called Sex, Drugs, and Rock and Roll.

[00:10:59] **Alex Volkov:** Oh, wow.

[00:11:00] **Eric Hartford:** this is the dataset because We wanted a vision model that doesn't shy away from, any kind of taboo materials. 

[00:11:09] **Eric Hartford:** The thing about cognitive computations, we're like the Eminem of, of AI. anything that's, controversial, that's what we're going for. And anything that is, taboo, that's what we're going for.

[00:11:21] **Alex Volkov:** we mentioned that LMSys released a Route LLM, a new open source router. we barely talked about that as well. and then, I think we moved our conversation into Nomic GPT4ALL, which is a 3. 0 version of their local LLM. Inference app, which is similar to LMSys. You can run local LLM models on your machine. And I think the most interesting thing there is just the incredible success for this open source project with over a hundred contributors, 65, 000 GitHub stars, a quarter of a million monthly users.

[00:11:56] **Alex Volkov:** And it's a private local LLM desktop app for everyday people. You don't have to know, like download CLI or Olama, et cetera. and I think the coolest thing that I highlighted is that they have the ability to monitor a folder on your Mac and you can drop PDF in the folder. They will auto embed those PDFs for you and then create basically a RAG system for you.

[00:12:18] **Alex Volkov:** Without thinking about embedding, without doing anything, it will happen automatically. shout out to the NOMIC team about this release.

[00:12:24] **Alex Volkov:** it's really good. It's really fun to use this, app and I recommend folks using this. And last kind of conversation about open source is Microsoft open sourced something called GraphRag. And to help us understand this, we actually, invited a friend of the pod, a listener of the pod,

[00:12:39] **Alex Volkov:** CEO of Neo4j, Emil Efrem, to the stage. Because GraphREG is a concept I heard from Emil for the first time last week during AI Engineer Summit. Because Neo4j is a long time graph database. company, and now with GraphRag and the ability of LLMs to actually create graph databases, GraphRag is becoming like the hot new thing, of improving accuracy and also readability and understanding of your RAG systems.

[00:13:07] **Alex Volkov:** We had a quite a long conversation, a deep dive, if you will, into GraphRag as a concept. also Microsoft's implementation of this, but generally GraphRag as a concept with Emil, who's very knowledgeable about this. it's not every day we have, expert deep dives, but we try to at least, at least two times a month to have deep dives into a specific topic.

[00:13:25] **Emil Eifrem:** So why would you even do this in the first place?we see a couple of drivers to this. The first one is basically accuracy. And this is what you're talking about right now. In other words, On balance, you get a higher quality response out of your RAG architecture, because you start with a ve this is the typical pattern.

[00:13:44] **Alex Volkov:** very clear communicator. I really enjoyed talking with Emil. So if, if you haven't heard about GraphReg, definitely check it out, our conversation with Emil. In this week's buzz category, where I talk about Weights Biases, I mentioned that we have a new course.

[00:13:59] **Alex Volkov:** And I mentioned again that we have a courses platform in Weights Biases that some people consider that we're a courses company, which we're not. But we have a free courses platform, wandb.me/courses, or, I it's wandb.Courses as well. And this new course is all about prompting. It's called The Developer's Guide to LLM Prompting.

[00:14:18] **Alex Volkov:** Features Anish Shah, my colleague, and Theodore Danilovic from AutoGen. It will cover everything you need to get started with prompt engineering, from system prompts, structural techniques, to model specific strategies. It's going to be super cool. I previewed it a little bit. I Definitely worth checking out.

[00:14:35] **Alex Volkov:** Anish is a great communicator. Every time I listen to him, I hear something new to me and is very just awesome. Just, if you're into this, it's a free course, check it out. I will add this to the show notes. 

[00:14:46] **Alex Volkov:** In the big companies and LLMs, we talked about Perplexity announcing their new Pro Research mode, which does multi step reasoning significantly better and utilizes even, Follow up searches on previous results and intelligent actions, based on, step by step reasoning.

[00:15:03] **Alex Volkov:** and we gave some examples. And, and then we had two examples of companies rolling back their AI features. The first one was Figma, paused the rollout of their new AI text to design tool called MakeDesign. After a viral tweet, after a tweet went viral, Asking for a design of a weather app and getting in response the almost one to one example of the iOS weather app with its non standard features.

[00:15:31] **Alex Volkov:** And the CEO of Figma, Dylan Field, How did you decide to pause the rollout of this make design feature and the other rollout? I was the participant in this. The other rollout was, paused, of a feature called grok analysis. Where on XFK twitter, you would have a new button that says grok analysis, and it would send this message.

[00:15:53] **Alex Volkov:** Tweet into grok's interface and ask grok to analyze that tweet, it's context, it's quote tweets and replies and summarize the conversation around this tweet for you and can explain what's going on. and then I've gotten some quite unhinged results. And I posted about them. The co founder of XAI, Igor Babushkin, replied and said, acknowledge that these results were in fact unhinged and were not supposed to get released.

[00:16:19] **Alex Volkov:** And then they paused this release and now this button doesn't work anymore or doesn't show up. And I think the last thing that we've talked about, and it was super cool, is a company, a French company called Cute AI released Moshi. It's a, or about to release also open source, but released a demo of Moshi.

[00:16:36] **Alex Volkov:** It's a 7 billion parameter end to end voice capable model. [00:16:40] So similar to GPT 4. 0, the Omni version. You can talk to this on the chat and it can talk back. it can listen while it talks. it's a real time voice AI. The latency is incredible. It's 160 millisecond latency of the model and microphone to speaker.

[00:16:55] **Alex Volkov:** So from the time I speak to the time I listen to something, the answers is 200 milliseconds. compare this with. Google's research about, instant, perceiving instant actions, everything under 250 milliseconds is actually perceived as almost instant by humans. this feels like instant,uncanny as well.

[00:17:13] **Alex Volkov:** We've talked about this on stage.we dove a little bit into the architecture and the plans of these, of the small team to release Moshi, and the training. Ability of motion as well.


## [00:17:24] Open Source start

[00:17:24] **Alex Volkov:** And I think, as always, we'll start with Open Source. Let's get it started. and, okay, so two people already told me that I forgot something. 


## [00:17:52] Microsoft's PHI 3 Mini Update

[00:17:52] **Alex Volkov:** And the thing I forgot is Microsoft's new PHI 3 mini update. I wanna just get another friend of ours here, Bratowski, maybe to tell us about this, because I literally missed this week. Apparently, Microsoft also updated PHI 3.

[00:18:04] **Alex Volkov:** Which is a tiny model from Microsoft that we've previously discussed. so now, because, Bortowski just told me about this, please go ahead, Bortowski. Welcome to the stage, man. it's been a minute. what did they update in PHI 3? Let's start with this.

[00:18:17] **Bartowski:** My, my connection may cut out. I'm going, I'm driving right now, but, yes, they updated PHI 3 in place, the mini version, both 4K and 128K. And the difference in performance is actually quite shocking to the point where I still think they really did themselves a disservice to not rename it something different.

[00:18:35] **Alex Volkov:** Oh, wow. that big.

[00:18:38] **Bartowski:** Yeah, like it's almost double the, it matches 

[00:18:40] **Bartowski:** Medium now.

[00:18:42] **Alex Volkov:** so the small model, I got the same feedback from, in our green room. We have a green room where a bunch of speakers also talk. the same feedback, but almost a new model. So that's what you get there as well.

[00:18:55] **Alex Volkov:** Yeah, it's been very interesting in the tests and using it out so far and it seems like they did a lot of really good stuff with the system prompt and instruction following as well with structured output. Amazing. Awesome. Alright, so folks, if you're using fi three, you heard it here. Thanks tki. and thanks for joining us from your driving. Hopefully you're driving to somewhere fun on, on 1st of July. fi three, meaning from Microsoft got a significant boost in capabilities and upgrade to the point where it's almost a disservice.

[00:19:21] **Alex Volkov:** Anybody here on stage tried it out and wants to tell us like a one line summary also from their experience.

[00:19:32] **Maziyar Panahi:** did Finetune on top of i3 mini and when I saw them in Updating it silently, quietly in place. And the score was changing, as said. incredible, Holly. So I tried it. it was an impressive model already, so it was really hard to know the difference. I just really wanna know, what was the process of, did they, to a further fine tuning, was that a DPO?

[00:19:58] **Maziyar Panahi:** What was that? Because they didn't mention anything about it, It was clear that they saw the leader board, the new leader board by hug and face the version two with all the new benchmarks. And they were like, okay, maybe we should just adjust a little bit to trainings and the areas that we are missing.

[00:20:13] **Maziyar Panahi:** Like the math level five was really bad, so I think they improved that massively. And so I really love to know the process, like how they really approached this new, the new training. Is it like a total new training or is it like a further. Training, but I did try it. The model was as good as before, so it's not like they, God forbid, they cheated in the, scoring.

[00:20:33] **Maziyar Panahi:** No, they still the same model. It's very solid. It did great in RAG in a lot of reasoning.I think they just really improved the areas that was lacking.

[00:20:42] **Alex Volkov:** Awesome. it's great to hear. And, definitely this model has been debated among us, even PHI 2 was debated among us, whether or not, it's been over trained on metrics. So it's so great to, to see, and a silent update. And it's great to see that, Multiple folks in our community have noticed this and said, Hey, we should at least, we should not skip this.

[00:21:01] **Alex Volkov:** We should mention this. So folks, no. So if you're using PHI3, you've got to boost. All right, folks, let's skip forward. 


## [00:21:07] InternLM: The New Open Source Leader

[00:21:07] **Alex Volkov:** We want to talk about InternLM. InternLM is now, according to, to, to the research, is the best open source model under 12 billion parameters on Hug and Face. they show us a state of the art performance on math reasoning.

[00:21:20] **Alex Volkov:** Surpassing, Lama3 and Gemma2, 9 billion parameters. and the whopping thing is, they have a 1 million context window support. With, what they claim is perfect, nearly perfect, nearly a haystack result. they released a base and chat and the chat 1 million versions and also, and we're going to talk about tool use soon, with Lucas as well, but they also released a stronger tool use and a agentic framework together to go with this.

[00:21:47] **Alex Volkov:** They call Legend. So they have a genetic framework that utilizes this tool use. And they say that,if you use their tool use capabilities with something like a code interpreter, they can match GPT 4 on several like tasks, math related tasks, with this 7 billion parameter model, which is quite incredible.

[00:22:08] **Alex Volkov:** Nisten, what do you think?

[00:22:11] **IDK:** Yeah, it is very good. I 

[00:22:14] **Nisten Tahiraj:** would just wait for an updated benchmark for the Neumark V, because that one just gets zero on the level five math, and I think that's just a problem with The prompting, or maybe needs a little bit of fine tuning because otherwise on MMLU PRO internal M gets 30 and PHI3 small gets 38.

[00:22:33] **Nisten Tahiraj:** So it gets a bit more into the factual recalls. But,yeah, no, this is good. the battle is heated 

[00:22:41] **IDK:** for like the most practical small one.

[00:22:43] **Alex Volkov:** Yep. And this is from Shanghai, I believe, right? Shanghai University, or something like, like that. so

[00:22:50] **IDK:** Yeah, they've been competitive before, so yeah.

[00:22:53] **Alex Volkov:** Yeah, Wolfram, go ahead, what's your take on Intern?

[00:22:58] **Wolfram Ravenwolf:** Yeah, I tested it today as well and my problem is because I need German or at least some kind of understanding there. That is a problem with the small models, but big context that's a good thing. So it's not useful for my users, but if I were to use my English or something like that, then I can see using it more.

[00:23:17] **Wolfram Ravenwolf:** But for me, unfortunately, I need bigger models that are better in German because my users don't speak English that well.

[00:23:26] **Alex Volkov:** Yeah, but you mentioned like the one million context window use cases. I don't know if That's incredible, right? I also don't know if I, if I'm able to run this 1 million contacts window at any point on any of my hardware. But as a concept, that just sounds incredible that they released it.

[00:23:42] **Alex Volkov:** and also, the agentic framework, which I haven't, I just looked at it today, honestly, I haven't been able to run this, but to release a model together with an energetic framework, that's also something super cool. And,thanks to our friends, we now have this pinned up to the tweet. Thanks, Muzzy.

[00:23:57] **Alex Volkov:** do you have any comments on this, Muzzy? Muzzy, have you looked at the Legend framework at all?

[00:24:04] **Maziyar Panahi:** yeah, actually,I saw the, how they are offering this, library and I liked that they, it comes with a code interpreter, the Python executor. So I really wanted to try that and it really fits well with their own model. So I was like, maybe it would be nice if it's. It could become another library that could be used by other local LLMs, but for now, it fits perfectly with their LLM.

[00:24:32] **Maziyar Panahi:** It did a great job in searching and a couple of tools that you added, like a calculator or calendar. I did manage to do parallel functions,the usual stuff, like what's the difference between, The degree of the weather with London and Paris. So it has to go and get them one by one. And at the end, do the calculator to give you the precise answer.

[00:24:53] **Maziyar Panahi:** I like it that they come with their own libraries because they have the LM deploy or LDeploy, I think, it helps you to [00:25:00] extend to 1 million contexts by offloading some of those things on CPU and memory, because it's impossible to find VRAM for that kind of thing. And I did test it on 128K because I just wanted to see.

[00:25:13] **Maziyar Panahi:** If I go head to head with the Quendo, Qwentu, on 7B because they are the same size, right? what will happen in the RAC situation and it performed perfectly the same thing as the, Qwentu,I might go higher because I would just want to see if it's going to be, because you do the needle in head stack, but it's not really as accurate benchmark as being in a real situation, right?

[00:25:36] **Maziyar Panahi:** how many documents it will miss? how many of them would be active? Totally wrong, right?but I do agree with Nisten. I would love to see a 34b or 70b. That would be like amazing.

[00:25:48] **Alex Volkov:** From Intern, specifically.

[00:25:50] **Maziyar Panahi:** Yeah, the same model, exactly the same model with the agent capabilities, with that large context. We are seeing very great models coming with the large context.

[00:26:02] **Alex Volkov:** so definitely a shout out to the team because they released the Legend framework. They released the model. They also released this LM deploy thing that you said for model serving. it's also there and also they released MagicDoc, which is a lightweight open source tool to convert multiple files, file types to markdown.

[00:26:18] **Alex Volkov:** So they released like a bunch of, tools around their model release. So shout out to this like crazy team for not only grabbing the top open source, place, but also releasing a bunch of, Surrounding open source tools. We love open source here at ThursdAI. So whoever's listens to this from internal, I'm shout out to this incredible push from everywhere.

[00:26:38] **Alex Volkov:** And you can download these models on Hanging Face, I believe as well. Go ahead.

[00:26:43] **Maziyar Panahi:** Yeah, just not to forget, they also released the base model. So it means that A series of nice, decent, fine tuned models could come. They DM'd me, I told them I would like to fine tune, they welcomed it. So they know what they're really doing. So thank you for releasing everything and sharing it with the community.

[00:27:02] **Alex Volkov:** Yep, thank you. And then DM them back and tell them that we appreciate this from the ThursdAI community. Welcome to send them the recording of this podcast. I think we're moving on, folks. Yeah, I think that we've covered InterLM. So the best open source model now under 12 and supports. Even, Code Interpreter, which they also gave us, which is super, super cool.


## [00:27:20] Introducing RC Agent

[00:27:20] **Alex Volkov:** let's talk about, let's talk about RCAI and, let's talk with Lukas. Lukas, do you, would, do you want to do this, this announcement yourself? You work at RCAI, we know about RC, we've talked about RC multiple times. Charles Goddard works there, the author of MergeKit. tell us about RC Agent.

[00:27:38] **Lucas Atkins:** Yeah, so RC Agent is, to kind of,sum it up, is it's a Qwentune fine tuned basically. it's a merge of a model we did We released last week called RCSpark, which is a more generalist, overall fine tune. There's a merge of that back into their instruct, and then, we trained on top of that.

[00:28:00] **Lucas Atkins:** Specifically for tool use and function calling, and I think people are sleeping on Qwent2 a little bit too much still. those models are outstanding, and they're very malleable. The biggest thing we did a little bit differently than just using standard function calling and tool use specific data sets is I wanted to be very I wanted it to be able to do multiple calls in a row, and a lot of those datasets,that are out there right now are mostly single tool use kind of responses.

[00:28:35] **Lucas Atkins:** Hey, here's my question, and then the model would, here's the tool I'm going to use to answer your question, and then here's the answer to your question. I took, basically like 25, 000 samples from Glaive FC2. Andre worked it a little bit to extend it so that some of those questions required multiple calls in a row.

[00:28:56] **Lucas Atkins:** we're really happy with it. It performs really well. Again, a lot of that wouldn't be possible without Qwentu and the stuff that the Qwent team is doing.And it's, yeah, it's really a lovely model. And Fernando is in here, he works with me, at RC, as well as, Mazzy helped a pretty good amount with, figuring out what was our training pipeline gonna look like.

[00:29:18] **Lucas Atkins:** but yeah, it was overall a fast turnaround. Those are really good models. Okay.

[00:29:25] **Alex Volkov:** are not sleeping on Quentu here on ThursdAI. Junyang is a constant joiner. He's often a co host of this podcast as well. Quentu was announced here on stage at ThursdAI, so we're definitely not sleeping on Quentu here, but it's great to see.

[00:29:37] **Alex Volkov:** I don't know why Junyang, this is the one time that you didn't join and now we have two Finetunes. They're like incredible. So we're talking about RC Agent now. I wanted to talk about the function code capabilities specifically, and the tool router things. Could you expand a little bit on, on those, capabilities?

[00:29:54] **Alex Volkov:** Like why are they important?

[00:30:00] **Lucas Atkins:** One of the things we like to think about with any of these model releases. LMSys islike, how is it going to be delightful to use, but also what's the business use case for it? And a lot of those, most of these, if you think of what is the business using an LLM for? Is it internal?

[00:30:18] **Lucas Atkins:** Is it external? in any situation, you're going to be aided by the ability to create and then utilize some kind of internal tool, whether that's calling information from a database, whether that's. transferring a conversation from oh, you're speaking with a chat agent, now we're going to send you over to a human agent.

[00:30:38] **Lucas Atkins:** Being able to do those kind of calls on models that you run entirely on your own servers and your own VPC, that's really important. And I think that just having the ability to have a model that can use a code interpreter and do most of the things as far as function calling and tool use that, we think are More sacred to these larger, more,closed off models.

[00:31:11] **Lucas Atkins:** I think being able to say, hey, here's an Apache 2. 0 release model. You're free to do it the way you want. Fine tune it however you need it to be. but you can throw it in. I did a demo, we'll probably release it sometime today, of basically turning it into a travel agent where, you know, you ask it, you tell it where you're going on vacation and then it does Wikipedia and Google searches to find you the best information before, you know,planning out your trip for you.

[00:31:35] **Lucas Atkins:** And that's just, that goes, it goes a step beyond just, plopping up a really nice AI as a chatbot and then having it try to recall from, several months to several years worth of out of date information. Yeah,

[00:32:18] **Alex Volkov:** thing that I did want to say. you presented, I think in your tweet, the, the comparison on the fine tuning benchmark from Berkeley. Is that the Gorilla benchmark thing?

[00:32:28] **IDK:** Yeah, so now your RC agent, which is a 7B parameter model, stands above on that benchmark parameter, on that benchmark for tool use parameter, RC agent stands above GPT 3.

[00:32:39] **Alex Volkov:** 5 and, above CloudInstant and, the previous version, above, Some versions of Functionary and Gemini. It's quite incredible. So it gets like a fairly top score as well for a very tiny model. It's super, super cool. Just below Command R which is topping that specific benchmark.

[00:32:59] **Lucas Atkins:** Yeah, it's, we were really happy with those results, and it, the, you're gonna get the best performance with it doing,through an OpenAI compatible server and using,OpenAI's function calling format. That's where you're going to get the best results. But we, one of the things we did want to be sure that if you were just propping it up on your own and wanted to do prompt [00:33:20] based tool use, it's also really good at that as well.

[00:33:23] **Alex Volkov:** That's awesome. congrats on the work and thank you for releasing this in, you said Apache 2 license, right?

[00:33:27] **IDK:** Yeah.

[00:33:28] **Alex Volkov:** Awesome. Cognitive, please go ahead.

[00:33:32] **Eric Hartford:** Hi, it's Eric Hartford of Cognitive Computation. I wanted to ask Lucas about this Is the dataset open? Is there plans to release the dataset for that? yeah. I'll release 

[00:33:46] **Lucas Atkins:** that probably sometime next week. I just need to pretty it up a little bit. 

[00:33:50] **Lucas Atkins:** Okay. Can't wait to see it.

[00:33:52] **Alex Volkov:** Awesome. Thanks, Eric. and then Eric, let's, if already like you're here and we're talking, let's talk about what you guys released. 


## [00:33:58] Dolphin Vision: A New Era in AI

[00:33:58] **Alex Volkov:** So Cognitive Computations released Dolphin Vision 72b. And I think, Lucas, you got a shout out there as well. Stable Quan, our friend, also worked on this. Eric, you want to talk to us about,what's big with Dolphin Vision?

[00:34:09] **Alex Volkov:** I think it's also Quantum based, correct?

[00:34:12] **Eric Hartford:** Yes, and Quen 2 is the star of the show right now. Because there's no better model. This is like GPT 4 level. It's open. Open, Waits, GPT 4. We can do what we want with it, and it's so powerful, and it's multilingual, and it's everything, it's like the dream model. I love it. I'm enjoying Qwenn,working with Qwenn, and,we decided We wanted to focus on this.

[00:34:39] **Eric Hartford:** We wanted a big one because we noticed a lot of these vision models are just little. First off, I want to say that it was Stablequan. He did this. I didn't do it. I helped out. I was part of it. And also, I want to call out Lucas and Fernando who also were helping with it earlier on. Fernando, in particular, he created a dataset called Sex, Drugs, and Rock and Roll.

[00:35:03] **Alex Volkov:** Oh, wow.

[00:35:04] **Eric Hartford:** this is the dataset because We wanted a vision model that doesn't shy away from, any kind of taboo materials. It should be able to talk about any image. because all these vision models there, you show them something, they don't like, and they'll say, hey, we're not going to talk about that.

[00:35:21] **Eric Hartford:** we don't like that. We're not going to, we're not, we don't have any comment. And we wanted to go the other way. The thing about cognitive computations, we're like the Eminem of, of AI. anything that's, controversial, that's what we're going for. And anything that is, taboo, that's what we're going for.

[00:35:39] **Eric Hartford:** yeah, just think of us as the M&amp; M of AI.

[00:35:41] **Alex Volkov:** So unfiltered, uncensored models. that's what we're to expect from cognitive computations.

[00:35:47] **Eric Hartford:** it's not just about uncensored, it's about, really whatever is taboo. for example, the whole idea of a sentient AI, or, an AI that is a person, that is, they specifically train the models not to give that impression. Guess what? We're going to train models that do give that impression.

[00:36:03] **Alex Volkov:** Because We're gonna do whatever they don't do. and that's what we're about. we're about freedom. We're about, doing everything that they're scared to do is what we do.what can you tell us about, Dolphin Vision

[00:36:15] **Eric Hartford:** yeah. Dolphin Vision, it was, really StableKwan's baby, and, he wanted to do it under our brand,and so we worked together with him, I got some compute resources together, and, and we trained it on, the 72B, because most of these, vision models are, are too small, they're not all that smart, so I, so we wanted to do it on a big model, and make it actually smart.

[00:36:40] **Eric Hartford:** the 72B was the key, and that, Sex, Drugs, and Rock and Roll dataset that Fernando put together,and it just came together, and I want to thank, also,Prince Kanuma, who's here, actually, for making the MLX version, so it will run on, MacBook, and, and I really hope for an OLAMA version soon, and, I, I haven't heard anything from them, but I'm bothering them about it.

[00:37:03] **Eric Hartford:** So

[00:37:03] **Alex Volkov:** they're probably going to release something. shout out to Prince Konuma because, you guys, he's in the audience here as well, but in pretty much every model release you'll see like after two hours Prince Konuma is like, Hey, here's the MLX version, which is super cool and great service to the community.

[00:37:16] **Alex Volkov:** Prince Konuma, shout out. Eric, we've talked about this on the show multiple times with folks who develop, larger models. Nisten, you remember this. How, maybe Nisten, you can take this one. How does like a bigger model. Bigger language model help a vision model become smarter.

[00:37:34] **IDK:** it was also, ang that,

[00:37:37] **Nisten Tahiraj:** talked about

[00:37:38] **Alex Volkov:** Yeah, Jun Yang from

[00:37:39] **Nisten Tahiraj:** and,yeah. Ang the dev lead from Qwenn. And,it just gets more time to, to chew on the. on the embeddings that come from the vision side. So like the vision brain feeds it into the language brain and I just have a bigger language brain to process what these abstract embeddings are all about and by having a bigger or smarter it doesn't necessarily have to be bigger but but just having more time more layers to process it you'll just get much better reasoned out output.

[00:38:13] **Alex Volkov:** Yeah. And so we've seen this, yeah, we've seen this multiple times already. So it's awesome to have now this huge, vision model and it's based on LavaNext. And, you got Dolphin in there. You have this,data set, sex, drugs, and rock and roll. Lucas, go ahead.

[00:38:30] **Lucas Atkins:** that, the sex, drugs, and rock and roll data set is key. But that was one of the interesting things about making a dolphin variant of a vision model is. Most of the Dolphin models, the language models, are quote unquote uncensored, not due to injecting any toxicity into the data set, it's done by removing refusals, right?

[00:38:53] **Lucas Atkins:** So you basically teach the model, you don't teach it to say no, and then it'll be more forthcoming, but then that means it can draw on, the vast amount of text data. It was trained on, even if you didn't explicitly train it to, to act in a certain, uncensored way. That wasn't going to be the case for a vision model because the number of vision, of image tokens that it was seeing compared to the text tokens is so drastically,orders of magnitude smaller.

[00:39:24] **Lucas Atkins:** and it also didn't see examples of any kind of, most of the stuff in there was, if it was an explicit image, it was, purposely for the use of creating a refusal. that was a very interesting challenge of, okay, how do you, again, uncensor a vision model? And so,it's really cool that, Again, the Sex, Drugs, and Rock and Roll dataset is awesome, and it was definitely key, but the percentage of total,samples that was, and,compared to the entire size of the vision dataset is pretty small, so it's good to know, and there's some future research to be done there about, okay, what happens when you negate all refusals in a vision model, so 

[00:40:10] **IDK:** it's promising.

[00:40:11] **Alex Volkov:** That's awesome. Thanks Lukas for the additional color on this and then contributions to this work as well and Cognitive, maybe last thing, Eric, from you, what is the license, how can people use this? And then maybe whatever else you want to add in terms of, just the floor is yours here.

[00:40:27] **Eric Hartford:** Qwenn has, for the 72B, they have their own custom license, as I understand, and that license, of course, is inherited by Dolphin Vision.so,I think it's, they have, I, I don't, I couldn't really comment. they, I think they have, you can't use it with the military, and stuff, kind of stuff like that, but it's mostly,permissive.

[00:40:47] **Eric Hartford:** businesses can use it. They don't have to pay. as my understanding.I also noticed that Pablo, I also wanted to notice that Pablo is here and I wanted to thank Pablo for doing the UI for Dolphin. it's really awesome. It would be cool to get Dolphin Vision in there if it's possible.thank you.

[00:41:08] **Eric Hartford:** can people use this right now? Is that, is it hosted anywhere? Should it be hosted? You said open.

[00:41:14] **Eric Hartford:** VulkanVision is not hosted, but I put up a, some code for, uh,a rep, not, a streamlet, I put some streamlet code where it's a UI, and you can drop an image in there and talk to it, but yeah, you gotta rent a run pod or something to run it. it, MLX, if you've got a Mac, you can run it on MLX.

[00:41:37] **Eric Hartford:** And right now it's not running on OLAMA. [00:41:40] I don't think so. I don't know if anyone made GGUFs of it yet.Anybody know? Did you, Bart? 

[00:41:48] **Nisten Tahiraj:** No, I've kept track of the REPL, and they still haven't made GGUFs of Phi3Vision either. There's quite a bit of work, cause that, the code that makes, the lava surgery is pretty old now, and needs to be rewritten, and they're kinda getting there, so it might take another, y'know, year.

[00:42:12] **Nisten Tahiraj:** Two, three weeks until that rewrite's done, then people can do better to GFs.

[00:42:16] **Alex Volkov:** Yeah, we have

[00:42:16] **IDK:** Or I don't know, it might be, might just be done in the last hour or two. I don't know.

[00:42:21] **Alex Volkov:** Yeah, we move fast here. there's a question from the audience, whether this can be used in LM Studio, which is,we like LM Studio. other vision models, I think Lava and other vision models could be used. So a good question is, could that be used there? But I think we need a GGEF to be able to run this.

[00:42:39] **IDK:** Yeah, 

[00:42:39] **Lucas Atkins:** once there's a goo, it'll work. 

[00:42:43] **Eric Hartford:** I that's where my knowledge drops off at the distribution side. so there's other experts 

[00:42:47] **IDK:** there.

[00:42:48] **Alex Volkov:** Alright, we're moving on. just on, on this topic by the way, we're staying like around this topic of how to use models locally. 


## [00:42:54] GPT4ALL: Local AI Revolution

[00:42:54] **Alex Volkov:** Our friends at NOC released GPT for All. So we talked about Elm Studio. before M Studio came out, there was another, run models locally on your Mac.

[00:43:02] **Alex Volkov:** It was called GPT for all from noc. And, our friends at No, just really released a revamped version of this. LMStudio is now open source. GPT4. 0 is open source, from a year ago. And so it has over a hundred contributors, 65, 000 GitHub stars. They have 250, 000 monthly users. And then now they just released a complete revamp of their UI.

[00:43:22] **Alex Volkov:** And it just looks way better, has dark mode, light mode, supports all of the same models. And so you can actually, and that's what I do, you can actually download the models via LM Studio, and then point GPT4ALL to that same directory, so you don't have to download models twice. And GPT4ALL will pick up all the same models as well.

[00:43:41] **Alex Volkov:** Why, you may ask, why would you do this if they offer the same functionalities? GPT4ALL, because it's from NOMIC, Nomec released multiple embedding models, for example, Nomec Embed, etc. they have a feature that I haven't seen in, LMStudio. Maybe it's there, but I haven't seen it. Which you can point it at a folder.

[00:43:58] **Alex Volkov:** You can say, hey, whatever documents is in this folder, automatically you watch this folder and embed them, and then automatically add them to my context for whatever model I'm using. So basically a local rag. With whatever context I want. I think that feature is super, super cool. And I just wanted to shout out Andrey, Milar, and the whole GPT4 all team.

[00:44:19] **Alex Volkov:** For first of all being like fully open source. We really like open source. But also giving us like this ability to have a private Talk to your documents, talk to your chats, any paper you can download from, from archive, whatever way we pronounce this. You can just drop them in the folder, gpt4all will monitor a folder for changes, embed them, and then you immediately get a rag, a ragable, whatever model you download from the hub.

[00:44:43] **Alex Volkov:** conversation with that, with the content of that folder. It's super, super cool. and you don't have to set up anything. It's all private and local runs on your device. You can use this, as we said, during a flight without the internet, it's really dope. So shout out, if you haven't yet run any models locally, GPT4O is a very good option, definitely give it a try and, everybody who contributed.

[00:45:04] **Alex Volkov:** Thank you. And it's available on nomic. ai slash gpt4all, with a four character, not a four, word. Anybody try gpt4all, the new one? And, has an impression of the folder, the live embedded folder?

[00:45:21] **Alex Volkov:** Besides me? No? Okay, cool. and I think that we're moving on towards our conversation about, if we already mentioned RAG, I think, the last thing that we are going to talk about, and I think, Emil, this is our conversation. We're moving towards a conversation about, other versions of RAG. So I think everybody in the audience knows what Reg is, but just in case any, anybody new is listening and I've heard this feedback before, I always introduce my podcast to like friends who are not from our bubble and they listen to one episode and they're like, Alex, I didn't understand anything.

[00:45:53] **Alex Volkov:** And you may want to explain more concepts. And I told them like, I can't always explain what RAG is in every episode, because if I explain what RAG is in every episode, everybody's gonna get super bored and will not listen to anything. if we talk about what Finetuning is in every episode, where the best Finetuners in the world join every episode, it's gonna be boring for the people, they will stop joining.

[00:46:12] **Alex Volkov:** there's definitely this balance between explaining concepts in the show that talks about this for every, the whole year, every week. but in case, just in case, somebody new joined and doesn't know what RAG is stands for Retrieval Augmented Generation. basically the ability to provide your LLM with additional context that it may not have known during its training or pre training or fine tuning, periods, from the dataset it was trained on.

[00:46:37] **Alex Volkov:** For example, your company's data, for example, specific data that happened after the cut of, cut of time of the model's training, for example, real time information,So stuff like this. And there have been new approaches into this. There obviously have been a lot of developments. we just came from the ai.

[00:46:54] **Alex Volkov:** engineer summit and like rag was the, one of the bells of the ball, everybody's talking about rag for some reason. This concept, this abbreviation broke through our bubble into the enterprise head bubble and so that's like everybody, every CEO of every fortune 500 company, like everybody talks about like RAG is the concept that they know.

[00:47:15] **Alex Volkov:** Finetuning lags, but now gets there as well, but I think we're at like right there way before finetuning, significantly before finetuning as well. And so now there are updates there as well. 


## [00:47:26] Exploring GraphRAG with Emil Eifrem

[00:47:26] **Alex Volkov:** So I want to introduce Emil Eifrem to the show. Emil, welcome. Please feel free to unmute and just first of all introduce yourself, who you are, and then we're gonna talk about some new concepts in this area.

[00:47:38] **IDK:** Hey 

[00:47:38] **Emil Eifrem:** Alex, super great to be here. as I told you last week, I've been an ardent listener of the pod. it was fantastic to meet face to face and in person. so my name is Emil Eifrem, I'm the founder and CEO of a company called Neo4j. Neo4j is a graph database, I think I can humbly say we are the OG graph database.

[00:47:59] **Emil Eifrem:** I've been around for north of a decade, so we're the same generation as a MongoDB or an Elastic, or something like this. and obviously graph database is a great, type of implementation or a great type of engine for storing knowledge graphs, which is the wedge into RAG and GraphRAM.

[00:48:19] **Alex Volkov:** Yeah, so folks who are listening now, first of all, Emil, thank you so much for being the listener for the show. And it was, I was very humbled to hear that you're listening and tuning in and it was great meeting you as well. But folks who are listening now, maybe hearing and Hey, Alex, how come you started in the open source area?

[00:48:36] **Alex Volkov:** And now you're talking about, with the CEO of a company that's been around for 10 years. what's the connection? The connection is Microsoft open sourced GraphRag this week, a paper and a GitHub repo about implementing a rag with a different way with GraphDatabase. And GraphRag is a concept I heard from you.

[00:48:52] **Alex Volkov:** For the first time in my life, last week during AI engineer, because you guys have also implemented something, with GraphRag and you're building on top of the graph database that you have. and I would love, and you explained it to me very simply, I would love, to ask you to do the same explanation to the folks in the audience.

[00:49:08] **Alex Volkov:** What is GraphRag? And then maybe we can talk about what Microsoft released versus what you guys have. that would be great.

[00:49:16] **Emil Eifrem:** yeah, so there's multiple layers to it, obviously, but at the absolute highest level, queuing off of what you said, that Maybe let's not just speak to inside the tent, but even to new listeners, right? Assuming that you know what RAG is, which I think we can assume most folks and you explain it really well, right?

[00:49:31] **Emil Eifrem:** Graph RAG is basically a RAG architecture where the retrieval path, so the R, includes a knowledge graph. So it's as simple as that. And it's not that or vector databases. The much more common pattern is that it uses a knowledge graph in addition to a vector index. And sometimes that vector index is served out of the graph database and sometimes it's a dedicated vector store.

[00:49:57] **Emil Eifrem:** So that's, at the highest level. So [00:50:00] basically, you use the knowledge graph to get better context and get higher quality responses. out of your RAG architecture. And we can go into all kinds of details, about how to do that and why would you do that, what are the pros and the cons, all that kind of stuff.

[00:50:14] **Emil Eifrem:** But in a nutshell, that's what GraphRAG is.

[00:50:17] **Alex Volkov:** Yep, and so I want to understand a little bit of the benefits of using something like a graph database. As you explained to me, I remember, let's say not versus a vector database, but, compared to, which is pretty much the same thing as now I realized after having said this, compared to a vector database in terms of like retrieval and understanding and maybe the ability to,to summarize larger or overall looks of, Over the whole document, for example, because as I understand it from the Microsoft release of their GraphRag, and maybe we can start there.

[00:50:50] **Alex Volkov:** Microsoft definitely announced that their implementation of GraphRag in the paper and also in the code

[00:50:58] **IDK:** The

[00:51:01] **Alex Volkov:** queries, for example, queries that talk about what are the major themes in a big document. Rag, as it stands for like vector search, because of the way chunking works, and chunking is just you just break down the document into smaller chunks, you embed them, and then you do a similarity search on them, because of the way chunking works, that type of query, the global type of query, like what are the main themes across this whole document, are, regular rag is not like that great to answer those questions, and because of the way graph database works, And the community search stuff, which I don't fully understand, to be very honest.

[00:51:37] **Alex Volkov:** it's possible to use GraphRec to answer those type of queries. could you help me out there to wrap my head around why,and why is it better for those type of queries?

[00:51:47] **Emil Eifrem:** Yeah, I think that's an accurate statement. So let's take a step back. what are the benefits? Why would we do this in the first place? so we, you just talked about GPG for all and how, hey. I can just point it to my local directory, like some folder, I dump a bunch of PDF files in there, and I get my RAG pipeline just automatically from scratch, right?

[00:52:06] **Emil Eifrem:** So why would you even do this in the first place?we see a couple of drivers to this. The first one is basically accuracy. And this is what you're talking about right now. In other words, On balance, you get a higher quality response out of your RAG architecture, because you start with a ve this is the typical pattern.

[00:52:25] **Emil Eifrem:** It's not there's many graph RAG patterns, but the most common one is that you start with a vector search, right? You get the chunks back out of some kind of vector index. Again, it might be a dedicated vector database, or it might be a vector index as part of, an existing graph database.

[00:52:41] **Emil Eifrem:** You get the chunks back, But then, you don't just return them to the LLM the way you would do with a vector only RAG. But instead, you take that and you traverse the context of that. And so you get more information back, more relevant information back. And through that, you get a higher quality response, from the LLM.

[00:53:00] **Emil Eifrem:** And there's been a number of papers around this. The first one was, I think, in December, maybe November of last year, the data. world paper, which showed a 3x improvement in inaccuracy with GraphRag versus vector only RAG. but even as I put together the slides for the AI Engineer Summit last week,there's like over that weekend, there was like two or three papers being published.

[00:53:22] **Emil Eifrem:** One from LinkedIn, and obviously then the Microsoft GraphRag one came out in, in February, the blog post and the article was published. maybe a month or two later, and then the code was released last week.and their particular angle on this, how do they, increase accuracy is exactly as you say, which is, all right, I'm going to do some pre compute, some graph algorithmic pre compute to identify communities.

[00:53:46] **Emil Eifrem:** out of the source data. And through that, I'll be able to answer more holistic information out of the corpus than you could do if you only see the individual chunks, right? So that's another version of accuracy. and what particular kind of angle of accuracy that you get depends on which graph rag pattern you use.

[00:54:06] **Emil Eifrem:** And so that's the first driver here. It's improved accuracy. But there's other things too and, you and I talked about some of them last week around improved developer productivity and things like that. But, if we stick around in accuracy, that's the highest level of what's

[00:54:19] **Alex Volkov:** Yeah, I would love to get into developer productivity because definitely there's eye opening things there as it relates also to the stuff that we're doing with, just observability. But my question to you is Just from the embeddings perspective, and we have, quite a few folks who are like embedding experts in the crowd here.

[00:54:37] **Alex Volkov:** from the embeddings perspective for vector databases, there, there needs to be an embeddings model. either no embed or folks use open AI embedding, et cetera. or Gina Embeddings. Shout out to bo here in the audience, our resident medical expert, to turn these chunks into embedding vector representations and then to be able to compare.

[00:54:57] **Alex Volkov:** vector similarities, in order to say, okay, what is the most similar to my query? This is basically how Reg works. graph databases are all text, as you explained them to me, with nodes and basically like a graph. The pre processing that you mentioned for Microsoft, are they using an LLM also to identify?

[00:55:14] **Alex Volkov:** So is there, the processing also uses AI to create those graphs? Like how does my document turn into the graph?

[00:55:23] **Emil Eifrem:** Yeah, that's a great question. So we're already getting to the stumbling blocks. We've talked about one of the drivers, one of the benefits, which is accuracy. Okay. So on some level, accuracy is another way of saying quality of response, and that's what everyone is looking for. If this is always better, why doesn't everyone do it all the time, right?

[00:55:40] **Emil Eifrem:** exactly as you're asking, like, how do you get the graph in the first place, right? You start out with a bunch of maybe PDF files, some kind of unstructured data, like how do you get the graph, right? And this is where the kind of LLMs come in here, right? Because all of a sudden, there's been a ton of research on this for decades, right?

[00:55:58] **Emil Eifrem:** called entity resolution and named entity extraction, all kinds of stuff, which basically tries to get out of raw. let's call it text for now. Out of raw text, how do I create a graph of those concepts, right? And the LLMs are sometimes really good at that, but of course it's like with everything LLMs, it works sometimes, right?

[00:56:20] **Emil Eifrem:** And so you actually, there's a real art in it, right? But it turns out that when you give it a little bit of schema, so a little bit of a concept of what's in those PDF files, right?I was talking to, like a user, a couple of weeks ago, which is a, like a big energy company, right?

[00:56:36] **Emil Eifrem:** A big oil company. And they have all the manuals of their oil platforms, pipelines, and, that kind of stuff, right? That's not generic PDF files. They know what's in there. It is exactly what I said, oil platforms, it's pipelines, it's engines, it's, I have no idea what's in there, but like that kind of stuff, right?

[00:56:54] **Emil Eifrem:** And so if you tell the LLM a little bit of that, and in the graph world we tend to call that ontologies, because we love to make up words and make everything sound harder than it is, but really it is just, it's a schema, right? Just alright, hey, assuming these are the things in the PDF files, please extract a graph of these concepts.

[00:57:14] **Adam Silverman:** Right? 

[00:57:14] **Emil Eifrem:** and that's the new step function changing capability that has allowed us to get to this next level of knowledge graph popularity. So that's the fundamental thing. Then there's all kinds of nuance behind it. There's unstructured data, there's structured data, and then there's the mixed data.

[00:57:31] **Emil Eifrem:** kind of data, which is actually the most common one. This is the one that people tend to call semi structured data, but that's actually not the right term for it. But that doesn't matter. But that's actually the most common one in real world production deployments. And then you have to use like a knowledge graph creation scheme, which involves both the structured information and the semi structured information,by help of the LLM.

[00:57:53] **Emil Eifrem:** But all of that leads up to, okay, so you have your knowledge graph up and running and that's when you can start using it in

[00:57:59] **Alex Volkov:** so step number one, like you go through the embedding angle to create a vector database, you have to either. You have to chunk and then embed using an embedding model of some sort and then store it either locally or whatever vector database. with GraphRec, you have to turn unstructured data into structure just in graph.

[00:58:17] **Alex Volkov:** and that also uses a model, but an [00:58:20] LLM and it's helpful with some structure. I just love, absolutely love the fact that, LLMs now are used In multiple levels of application in production, not only as a, okay, LLM answers the user, but also LLM prepares the data database itself, the schema for the database itself, but also the kind of the entity resolution in the database itself.

[00:58:42] **Alex Volkov:** because we also talked about just like LLMs all the way down, because we also talked about Sakana AI, I believe, and their LLMs are used to come up with better preference algorithms for datasets, for their like fine tuned versions also. So just like LLMs are in every possible level now of creating models themselves and applications.

[00:59:05] **Alex Volkov:** so that's incredible to me. Emil, we talked about some developer benefits as well. So we, accuracy, obviously it's for accuracy. At least for some queries perform significantly better for those who traverse the whole context, let's say of a document. what are some benefits of GraphReg on top of, let's say, just vector search?

[00:59:29] **Alex Volkov:** this was one of the more surprising thing. Like when we went all in on this about a year ago or something like that, right? and we started experimenting with this and talked to a bunch of users, right? one of the most surprising things that people told us was that, you know what, honestly, it took us a while to create the knowledge graph, right?

[00:59:46] **Emil Eifrem:** Hey, let's not mince words here, right? hey, we just wrapped our head around the chunking, and it was so easy to do chunking for our vector index, for the POCs and the small kind of hello world ish type things. Dugan in production turned out to be really hard. We finally wrapped our head around it.

[01:00:04] **Emil Eifrem:** And then, all of a sudden, you have to create a knowledge graph too. Okay, cool. So that took us a while to get around. But! Once we had that up and running, all of a sudden, our data came to life. Our data was clear and visible. And so this is where it was a lot more easier to show you this, than trying to give a, like a verbal description of it.

[01:00:25] **Emil Eifrem:** But you can imagine that if you have your data expressed that there's a node here with a relationship to that node, and you can visualize it, you can see it. Then all of a sudden, as you sit there, and you're an AI engineer, right? Again, the target for most RAG architectures isn't the people who are, like, very comfortable doing fine tuning, for example.

[01:00:45] **Emil Eifrem:** They're typically, hey, yesterday I was a full stack engineer, now I'm learning this new kind of AI ecosystem, right? And all of a sudden, They can look at their data when they build it. And so we have had so many people come tell us that, hey, we ported it from a vector only architecture, and we actually fixed a bunch of bugs.

[01:01:03] **Emil Eifrem:** I showed you some of those quotes last week, right? And we already fixed a bunch of bugs because we can see the data, right? And that ends up being really powerful. Ultimately, vector space Graph space is very powerful for machines to, do some kind of Euclidean space, cosine similarity, type calculations, but it's very opaque for a human being.

[01:01:25] **Emil Eifrem:** Graph space is very explicit and clear. And that is the underlying driver of why once you have the knowledge graph in place, it's actually easier to build your RAG application, which is really cool too. This one doesn't get, by the way, as much of the limelight. People don't talk about it as much because it isn't as stark as, Holy crap, I can get better answers, right?

[01:01:47] **Emil Eifrem:** But honestly, like over the course of many years, I think this one might be as powerful or even more.

[01:01:55] **Alex Volkov:** where this connects to me is, Obviously, I work at Weights Biases. 


## [01:01:59] Understanding LLM Observability

[01:01:59] **Alex Volkov:** We have a tool for LLM observability. Observability, right now, what we can see is, what LLM answered, and then also for RAG applications, what chunks were returned, but not why chunks were returned. Why, this was returned, this similarity.

[01:02:15] **Alex Volkov:** And then what we talked about, and you told me, and I immediately asked, but yeah, but similarity, there's, And you said, but like, how do you know based on which dimension this was returned? And I stumbled. I don't know. There's 12, 000 dimensions, whatever. Go figure based on which 12, 000 of them, the most similar ones and what they represent.

[01:02:36] **Alex Volkov:** And it's basically impossible. 


## [01:02:38] Graph-Based Data Insights

[01:02:38] **Alex Volkov:** And you're saying in graph world, I can just see this, right? I can understand based on which kind of, uh.edges or so based on which nodes I was getting this exact data, correct?

[01:02:50] **IDK:** No, 

[01:02:50] **Emil Eifrem:** that's exactly right. so my, I think the example I used with you, I believe was something like, hey, let's say we have a green tennis ball and we have an apple, right? hey, there's going to be some kind of cosine proximity between the two and in. some kind of ANN type search, it might be returned very high up on that list, right?

[01:03:09] **Emil Eifrem:** So they're similar in vector space, right? but why? Is it the roundness of the thing? Is that the greenness of the thing? And like in the graph, here I have an apple and now I have an orange, as an example, right? Okay. They are related because they both belong to the concept of fruit, right?

[01:03:25] **Emil Eifrem:** So we know that they're related because of the fruitness of the thing. and again, this is not an XOR. This is not either or. This is using both in combination. It's really powerful.

[01:03:36] **Alex Volkov:** So that's an interesting point. And I want to send the both things. 


## [01:03:39] Hybrid Search Techniques

[01:03:39] **Alex Volkov:** So one of the topics that I heard as well in the AI engineer,and before that people suggest not only doing vector database search, but also doing plain text search for many things and doing like a hybrid. that is very helpful, for reg, you're saying a hybrid with vector and graph as well.

[01:03:58] **Alex Volkov:** So now, would all three of them together be the most powerful, you think?

[01:04:04] **Emil Eifrem:** I think that's probably true, right? I think that's probably where we're going to converge. I think, look, the R in RAG, we just said it, is retrieval. IR, information retrieval, like the biggest experiment humanity has run on IR is called the web. Right? And Google fixed that in late 90s, early 2000s by using a graph algorithm called PageRank, right?

[01:04:28] **Emil Eifrem:** In other words, what did they do? They used all kinds of signals, BM25, full text search, some kind of inverted index search algorithm, right? They used obviously all kinds of vector search, all kinds of stuff to get the best pages back, the best documents back. And then they use the topology of the graph to rank them to get to the top 10 blue links.

[01:04:51] **Emil Eifrem:** It seems very likely to me that RAG architectures are going to converge in the same way now Assuming we can get the developer experience there, right? And this is the thing that, I'm harping on all the time probably because I'm a mediocre developer these days right, like it's if you sit down and like many of these frameworks like are actually Fairly opaque to use and fairly hard to use and then Someone mentioned Streamlit before, that's very opaque.

[01:05:20] **Emil Eifrem:** And then you throw all the LLMs, the non deterministic LLMs in there, and you have opaque vector space. There's a lot of black boxes involved, right? And here I come and I say, hey, you know what? You should use knowledge graphs. But yeah, but let's make it easier to create them first, right? So I think that, I think, is the bigger game here.

[01:05:39] **Emil Eifrem:** Like, how can we make this technology, easier for people to adopt. If we are able to do that, we're probably going to converge on a similar solution to IR on the web, which is, hey, let's use some kind of text and hybrid vector search and use the graph for ranking. That's going to give us the best responses.

[01:05:57] **Alex Volkov:** That's awesome. Emil, I think we have two guests here who also want to ask questions, and then I'll get to there. So Nisten, go ahead.

[01:06:05] **Nisten Tahiraj:** Yeah. So before all this, LLM stuff, I was a TypeScript dev, and I, even during the LLM wrappers, I used pgVector on,and that worked pretty well, but I just remember from a couple of years ago that the queries for Neo4j were just really hard, or maybe I was just dumb back then, but they were just pretty, pretty hard to get right.

[01:06:31] **Nisten Tahiraj:** Has that Changed recently with the LLMs? I don't actually know because I haven't, looked at, I always wonder, and it's like a weird [01:06:40] moment in the Matrix now that it's happening. But ha has that improved a lot because of the, because of the LLMs? can you make now a really nice cur builder for Neo four J? 

[01:06:52] **Emil Eifrem:** Yeah. First of all, thank you for the matrix reference. That's actually the reason the company's called Neo four J. Because I love the Matrix. I wanted to call it Neo, but neo.com was too expensive, so I had to get Neo four J back in the days. So I love the Matrix reference. yeah, so basically, so our query language is called Cipher.

[01:07:10] **Emil Eifrem:** that has morphed into what's called GQL, which is the first standardized query language for databases since SQL in the eighties. It just got standardized by ISO a couple of months ago, actually, which is very cool. today what you can do that you couldn't do, sounds like that was a few years ago, is of course the copilot experience.

[01:07:29] **Emil Eifrem:** So one of the things that we're investing a ton in is like, all right, if you know what query you're going to ask, and you can express that in English, right? We should be able to now with these kind of magical black boxes called LLMs, we should be able to help you do that. So there's, and this is called, this is the equivalent of Text2SQL, but our thing is called Text2Cypher.

[01:07:49] **Emil Eifrem:** so that's the best way to, to learn Cypher these days. all

[01:07:54] **Alex Volkov:** That's awesome. and

[01:07:55] **Emil Eifrem:** for this evening's

[01:07:57] **Alex Volkov:** AI and like the complex things is definitely one developer experience. addition, but there are more and I think Emil, you should talk about them. But before this, Adam, welcome Adam from Agent Ops. I think you popped up because you have a question. You were listening to this.

[01:08:13] **Adam Silverman:** Yeah, no, I was, and thanks so much for the introduction, Alex. 


## [01:08:16] Building Practical AI Applications

[01:08:16] **Adam Silverman:** I think one of the big things that we like to think about is just really, narrowing the scope, of these applications. So I think a lot of, like, whenever this, emerging technology comes out, everybody's always just trying to figure out, okay, how do we actually apply use cases?

[01:08:26] **Adam Silverman:** How do we actually, start building, crazy applications from the ground up? and I think one thing that's, really important for kind of anyone listening, just to ground yourself in, what is the smallest application you can build? with this, like what is like the, the actual business outcomes you can start driving opposed to here's a pie in the sky, possibility that might be possible down the road.

[01:08:42] **Adam Silverman:** so that's just one thing I want to throw out there and one thing we always try to ground ourselves in whenever we see these big announcements.

[01:08:51] **Alex Volkov:** I just want to like recap, you're saying, what is the quick and dirty demo example of something like graph database that showcases the very clear benefits.

[01:09:03] **Adam Silverman:** Correct, and something that actually just works,that is actually works as opposed to, here's something that works on, 10 percent of use cases or 20 percent of use cases. Figure out something that, works actually just very reliably, and I think that's a very good starting point to actually.

[01:09:15] **Adam Silverman:** Build business use cases, build startups, build new ideas off these, emerging technologies. but it's one thing we always try to like ground people with, especially when they're like building agents and stuff like that. It's just, we don't have AGI yet. We don't have all encompassing from a single prompt, you can build super, super complex, applications.

[01:09:29] **Adam Silverman:** So really just whenever these new emerging, technologies come out, just try and figure out like what is like the simplest application you can build. They can actually go prove to a customer that they actually want to purchase it or actually will seek a ton of benefits

[01:09:40] **Alex Volkov:** Yeah, I think Emil, the quote that you showed me, and I think you present it on stage as well. And also the demo that you showed, those were the two things that, that

[01:09:50] **IDK:** That I finally showed.

[01:09:51] **Alex Volkov:** Yes, you finally showed. That was incredible. I think those two kind of materialize what Adam is talking about to me.

[01:09:57] **Alex Volkov:** do you, would you be so kind to repeat the quote? Because I think it was like super cool. and I think there's more to it than this, but the quote I think was about already being able to fix bugs, but also improving, improving accuracy, correct?

[01:10:13] **Emil Eifrem:** Yeah, so maybe a couple of things. So first of all, I really agree with that, hey, let's choose as narrow of a surface as possible that delivers real value, right? So I think that's a kind of a general kind of good approach when you build software. To me, it's. It's tightly correlated to time to value.

[01:10:29] **Emil Eifrem:** And this is something that if you think about vector only RAG, time to value is really fast, right? Because again, you point it to a, I guess these days, like a folder with a bunch of PDF files and it automatically creates all the embeddings and the chunking and, all that kind of stuff, right?

[01:10:46] **Emil Eifrem:** So what's the equivalent of that for us? 


## [01:10:48] Knowledge Graph Builder Demo

[01:10:48] **Emil Eifrem:** And that is the demo that Alex and I were talking about that I did on stage at the AI Engineers Conference, except my Wi Fi, of course, broke You know, in the, whatever, 15 minutes that I showed the slides until I went to the live demo, then of course I wasn't connected, but ultimately we made it work, and what we've done is we've built this thing that we call Knowledge Graph Builder, and it is what you would imagine, right?

[01:11:12] **Emil Eifrem:** It's basically you pop in a Wikipedia page, you pop in a PDF article, you pop in a YouTube clip,you can put your S3 buckets, anything like that, and it's going to take that and even without any hints about schema, it'll create that first knowledge graph for you. And then, optionally, you can also, again, coming back to my example with a, Oil Gas Company.

[01:11:35] **Emil Eifrem:** If you know that the PDF files include, some particular concepts, you can also spell that out to help the LLM out, right? And that'll create the knowledge graph for you in a very visual way, automatically, right? And then you can sit there and you can chat with it and it's not a very kind of production quality bot, but at least you can use a graph rag out of the box and you can visually inspect the responses back from it.

[01:11:58] **Emil Eifrem:** so that to me is. is tightly correlated to, hey, let's choose a narrow surface that delivers value. Okay, but what's the time to value for the technology stack to get to that business value? And the Knowledge Graph Builder, I think, is a great way of doing that.

[01:12:14] **Alex Volkov:** Yeah, I want,

[01:12:14] **IDK:** It is.

[01:12:15] **Alex Volkov:** play with this. sorry Nissen, one second. I want to play with this and I don't know if I took the screenshot of the URL and, but you said you deployed it, not like production wise, you deployed it for just, a demo. Will that be deployed production wise or like for folks to play around with?

[01:12:28] **Alex Volkov:** like what's the plans for the builder? Is that going to be built into the Neo4j platform?

[01:12:33] **Emil Eifrem:** yeah, it will, and the Knowledge Graph Builder will be production quality, right? And it's all, it's already pretty close to that, actually, for a lot of data sets. Again. LLMs are involved, so there's some non determinism involved. so what I'm talking about, the kind of the demo quality support, it also includes a really simple chatbot where you can immediately chat with your data.

[01:12:55] **Emil Eifrem:** But it's really neat because you get the graph back, as it's traversing the graph and figuring out the answers, right? They're figuring out, let's be more specific, figuring out what data it's going to pass on to the LLM for the generation phase of RAG, right? It'll visualize that and show that in the graph form, right?

[01:13:13] **Emil Eifrem:** And that one is pretty slow because we're paying for all the tokens. so that's the only kind of demo part of it. But. But in terms of the pipeline for creating the knowledge graph, it's actually pretty solid already today.

[01:13:25] **Alex Volkov:** That's awesome. so Emil, I want to be conscious of your time. Also, it's a holiday, and, maybe two more questions and then we'll let you go. we have Alison here. Please go ahead.

[01:13:34] **IDK:** Yeah, I had 


## [01:13:35] Ontology and Graph Extraction

[01:13:35] **Allison:** one question specifically around the ontology and graph extraction. Have you found any specific types of documents or data that, work really well with extraction into ontologies? Or alternatively, any documents that just don't work as well or don't get as much of a benefit from being in a graph format?

[01:13:51] **Allison:** And then the second question, is actually into that and let me get back to that. 

[01:13:57] **Emil Eifrem:** Yeah, so here's, so let me maybe paint a spectrum here, and I alluded to this before, but if you think about kind of three core buckets of types of data, we have the completely structured data. which is, hey, it's already in Postgres or Snowflake or something like that, right?

[01:14:17] **Emil Eifrem:** Then going from that to a knowledge graph is really easy. there's a methodology people tend to call things like tables for labels, actually labels for tables, right? Where if it's a table in Snowflake called persons, we're going to create nodes with a person label on it. And every column in that table will be a property onto the noun, right?

[01:14:39] **Emil Eifrem:** So it's actually pretty easy to go from that, right? And then you have the middle bucket is the mixed data bucket. So this is structured data, but with long form text fields. So the long form text fields is typically where, hey, I have my comments if I'm building a Stack Overflow application, right? [01:15:00] Or where I have the entire support article text, right?

[01:15:04] **Emil Eifrem:** something like that, right? If I'm building a, customer service bot type thing, right? And then the third bucket is the completely unstructured. It could be any data. And for the completely unstructured, that's where it's the hardest. And that's again where you need to go in and help out the LLM as much as possible with the schema, in order to get the best knowledge graph created.

[01:15:25] **Emil Eifrem:** But the mixed one, that's by far the one that we see the most in real production quality, coming back to the previous slide. Previous discussion around, hey, what's the narrowest, product surface that delivers business value, for the startup or for the enterprise. The vast majority of use cases we see actually have the mixed data.

[01:15:45] **Emil Eifrem:** So it's structured data and kind of those long form text fields. and that's really where it's the easiest as well to create the knowledge map. So I'm not sure if that answered your question. 

[01:15:55] **Allison:** Yeah, that's perfect. Thanks so much. 


## [01:15:57] Latency in Graph Traversal

[01:15:57] **Allison:** last thing is, if you look at a traditional use case for vector search,hybrid search or something that low latency is really valuable in, how does walking the graph play into that latency?

[01:16:07] **Allison:** and is that something Neo4j, you found that you can fit into a lot of use cases without additional latency or users are complaining, that's something that's fine and that's not a problem? 

[01:16:17] **Emil Eifrem:** Yeah, we've not, this one was very top of mind for us when we first started working with this. okay, we're, on some level, we're adding an additional step here, like you vector search, you get the initial support articles, but then you graph traverse from there to find other related ones through the graph, right?

[01:16:35] **Emil Eifrem:** Clearly, that must add latency. sure it does. It turns out, in reality, all dwarfed by the LLM generation step. And I'm sure also kind of network overhead and stuff like that. Sohonestly, much as a surprise to us, this has not been like, no one has been concerned with this or given us feedback on, on this one, at least not yet.

[01:16:59] **Alex Volkov:** That's,

[01:17:00] **IDK:** Awesome. Thanks so much.

[01:17:01] **Alex Volkov:** thanks Alison for coming up and Adam for additional questions from the audience. Nisten, I think you have one last comment and then I'll wrap up this interview with Emil. Go ahead, Nisten.

[01:17:10] **Nisten Tahiraj:** Yeah, we've made fun of, enterprise retrieval augmentation systems that they should have just been a, a database query, but now it turns out they really can be. It's just one really nice query. You can replace an entire. an entire rack system with that and just pull in all the right sources and fill it in so yeah this is extremely interesting to me I think and yeah it's it's about time we started started putting some graph theory up in this. 

[01:17:40] **IDK:** Awesome, love it.


## [01:17:41] Final Q&amp;A and Closing Remarks

[01:17:41] **Alex Volkov:** Emil, I just wanted to, first of all, thank you for coming up. It's been great to discover that you listen to the show and then, having you very simply explain to me this like new concept and then seeing it everywhere after you explained this to me from Microsoft. Obviously, you also mentioned that other bigger companies build this kind of internally as well.

[01:17:58] **Alex Volkov:** And I don't think we've touched upon this, but like other companies, like Google, I think you said as well, some companies you guys work with that you still cannot show and tell, but we'll probably, at some point as these things go. where is, like where's Neo4j taking this GraphRack system, thing going forward?

[01:18:16] **Alex Volkov:** Where can people learn more about this? Where can they try out, tell us more about this thing? Like the floor is yours here to plug whatever you need to plug.

[01:18:24] **Emil Eifrem:** Yeah, so first of all of this that we do is coming back to your initial framing of this. All of this is open source for us. We've, since day one, always had a kind of a community edition, and now that we're a cloud service, there's a free forever tier of our cloud service called Aura, right? All of the GraphWrite stuff that we're doing, the frameworks, all the integrations, we have integrations with, man, I, of course cannot say all of it, but like a lot of, call it orchestration frameworks, the langchains and the LAMI indexes and the haystacks, like all of that is completely open source, and you can go to all of that.

[01:19:01] **Emil Eifrem:** We have a Neo4j. com slash lab slash GenAI ecosystem, I'll post the link later. we also are hosting a completely kind of database neutral, vendor neutral, GraphRag Discord, which is discord. gg. goodgame slash graphrag.so that's, there's a lot of high quality conversations, happening there, and overall I'm just really excited to see this take off.

[01:19:26] **Emil Eifrem:** I think this is an opportunity to make it easier for developers to build just much more powerful RAG architectures, which I think is really exciting.

[01:19:36] **Alex Volkov:** Yep, I completely agree with you there. And also, really, I'm really thankful for the ability to explain this as a new concept and for all of us to learn and my small part to play here as the question asker and the learning partner as well. Emil, thank you so much for taking the time out of your holiday, as I know you need to celebrate as well, to come here on Thursday and answer some of these.

[01:20:01] **Alex Volkov:** I appreciate that you did. And,

[01:20:04] **IDK:** It's been an honor.

[01:20:04] **Alex Volkov:** Yeah, it was awesome.

[01:20:05] **IDK:** Thanks for listening.

[01:20:06] **Alex Volkov:** Thank you so much. Thank you so much. 


## [01:20:08] Upcoming Courses and Announcements

[01:20:08] **Alex Volkov:** Alright, folks, I think we're moving on to our next topic. And our next topic is going to be, this week's buzz category. super quick. I'm going to plug because I think it's very interesting as well.

[01:20:16] **Alex Volkov:** we have a courses platform that I've talked about not a lot, but I will definitely bring this up more. I had somebody come up to me and actually say, you guys are a courses company, right? And I was like, no, we have a product that OpenAI uses and other foundational companies use to track their experiments.

[01:20:38] **Alex Volkov:** And he's oh, I thought you were making courses. So he was sure that Weights Biases is actually a courses company. it was really funny to me. I told this to the folks at Weights Biases who make courses and they, they were floored. this is basically my team. There's two people on my team.

[01:20:51] **Alex Volkov:** that kind of, collaborate with everybody else, like the machine learning engineers. So we have two new courses coming up. I'm going to talk to you about one of them. And then the other one, they're free, completely free. You can sign up and just go through them. The next one is called Developer's Guide to LLM Prompting.

[01:21:06] **Alex Volkov:** we'll cover everything you need to know to get started with prompt engineering. So if you're hearing about this concept of prompt engineering, you're not really sure if you've, if it's too late for you, if if the prompt engineering discourse moved on and you're like following up, this will be like the zero to one for you.

[01:21:22] **Alex Volkov:** From system prompts to structural techniques, like to model specific strategies, text understanding use cases. So experience with all of this and getting caught up to date with industry knowledge. Anish Shah from my team in Weights Biases and Theodora Danilovic from AutoGen AI are going to, teach you for free how to be a prompt engineer.

[01:21:41] **Alex Volkov:** And I'm going to add the link to the show notes. But basically, wandb. course. is the URL or Waits and Bites courses, just Google this, it's free. There's a bunch of them there. We usually collaborate with other like great companies on courses. now after chatting with Emil, maybe it's time for us to do something graph related as well, but for now we have this prompts course.

[01:22:01] **Alex Volkov:** And then we also have a, obviously we're in the. Observability and evaluation area with our product Weights Biases Weave, which I talked about at the AI engineer conference a week ago. And we also have a course about evaluations and alignment of LLM as a judge to human preferences. So that's coming up next week.

[01:22:22] **Alex Volkov:** I'm going to talk about this as well. So the developer's guide to LLM prompting course comes in four days. It's free. Definitely check it out. It's going to be Anish is great. He's a great educator. Like every time I listen to Anish, I learn a bunch of new stuff. even stuff that I covered already on ThursdAI.

[01:22:40] **Alex Volkov:** I think that we've covered pretty much everything in the open source. besides this one thing that's open source, but not yet really. 


## [01:22:46] CuteAI's Multimodal Model Demo

[01:22:46] **Alex Volkov:** Let's talk about CuteAI, LDJ, welcome to the stage. I think, we both got excited because CuteAI released something as like a pre announcement that they're going to talk about something and then you want to maybe talk about CuteAI and what they released or at least start the conversation.

[01:23:02] **Alex Volkov:** And then we're maybe going to do a live demo. We'll see if that works.

[01:23:07] **IDK:** Sure, and, 

[01:23:08] **LDJ:** I think it is pronounced Q Ti, but I'm not 100 percent sure myself,

[01:23:12] **Alex Volkov:** CuteAI, maybe yeah.

[01:23:13] **LDJ:** but yeah,I've been following this company for a while, I noticed that they had raised some significant money, I think around, 300 million. [01:23:20] Dollar funding round. They have some people from Meta DeepMind that worked on interesting computer vision and audio AI stuff.

[01:23:27] **LDJ:** Some of the people even from the Lama team and they said they're working on some multimodal

[01:23:31] **Alex Volkov:** Backed by Eric Schmidt as well. Yeah,

[01:23:38] **LDJ:** funding groups in Europe. And they ended up announcing, I think it was like a few days ago, hey, on Tuesday, at this time we're going to have this announcement showing some of the progress on our work.

[01:23:50] **LDJ:** So I'm like, hey, Alex, everybody, you guys should maybe check this out. And it ended up being this, GPT 4. 0 light, we could call it. And it's something that, it

[01:24:01] **Alex Volkov:** Very, like 7 billion parameter light. It's like tiny.

[01:24:05] **LDJ:** yep, they did announce that they're, they've been saying since the beginning that they're committed to open science and such.

[01:24:11] **LDJ:** They didn't, I don't think they used the word open source, but they did confirm at this newest event that they'll be making this model open source as well as releasing a paper on it. And I guess to summarize it, the most interesting bit about it is the fact that it's pre trained on both audio and text end to end, and it could take in raw audio essentially and output audio.

[01:24:31] **LDJ:** And this allows you to talk to it, and you can even listen and speak at the same time. And so just because it's speaking to you doesn't mean it's not able to hear you either, and yeah, they have a demo up online right now. I don't think they've opened up the weights online yet, but they said they are going to do that soon, along with the paper release.

[01:24:49] **Alex Volkov:** I think that the mind blowing thing that we all saw at the GPT 4. 0 demo, the O stands for Omni. The model was Omni, like it understands text and audio natively and is able to generate audio natively. This model is similar to that. It understands both you speaking to it. I don't think it has text though.

[01:25:10] **Alex Volkov:** it just has audio on the input. Correct, LDJ? I haven't seen it. Like you can't

[01:25:14] **LDJ:** I didn't see anybody typing to it, but I do remember hearing somebody saying that you could type to it, but I'm not

[01:25:20] **Alex Volkov:** Maybe the interface doesn't allow you, but it could be multimodal

[01:25:23] **LDJ:** say that it's, sorry, yeah, they did say that it's pre trained on both text and audio. yeah.

[01:25:29] **Alex Volkov:** And then,the most impressive stuff that I saw, and this is before the demo, the model latency is 160 milliseconds. And the microphone to speakers latency is 200 milliseconds streaming from the server, 200 milliseconds. Folks, Google, when I was a young front end developer, a long time ago, Google did the research about, latency.

[01:25:51] **Alex Volkov:** And they said that everything under 250 milliseconds, humans basically perceive as instantaneous. So like button clicks, responses from server, all of these things under 250 milliseconds, Humans perceive as instantaneous. These people. Built a 7 billion parameter model with, that they host on the server somewhere, that can run on your laptop.

[01:26:13] **Alex Volkov:** They showed it. This can run on like MLX, whatever. I see, I don't know if Prince Konuma is still in the audience, but like he's probably salivating right now. 7 billion parameter, end to end voice capable model that can run on your laptop, that can talk and you can talk to it, like bi directional, I think it's called full duplex or something.

[01:26:30] **Alex Volkov:** There's like a name for this. That the latency is like less than 200 milliseconds when it runs on the server, which is ridiculous. and it's only like a bunch of people. It took them less than six months to create this. it's constructed out of an LLM. They call them, they call it a Helium 7B.

[01:26:48] **Alex Volkov:** they did a lot of work on like audio codecs. they have a new like proprietary, I think, codec called Mimi. And they have their own infrastack. Inference stack,based on Rust. all of this information is from our friend VB at Hug Face. the, and,potentially this is a candle from Hug Face, which is a Rust, inference stack.

[01:27:06] **Alex Volkov:** and, like LDJ said, it will be open sourced, which is incredible. Wolfram, go ahead, and then we'll try to do a demo.

[01:27:13] **Wolfram Ravenwolf:** Yeah, the latency is so good that it was pretty uncanny because I was asking a question and it started answering before I had realized I was finished asking the question. So that's what you said with 250 milliseconds or less. It feels so instant that, yeah, it was really, disconcerting

[01:27:30] **Alex Volkov:** It's even too fast, yeah. It's even too fast. It's so weird. It's so weird. the interability, so they,I think the biggest part of GPT 4. 0 thing is that you guys saw, like, all of the OpenAI people, once it started talking, they said, wait, wait, wait, wait, wait, and then it stopped. Or wait, just a second, they started talking, GPT 4.

[01:27:50] **Alex Volkov:** 0 stopped and shut up. This doesn't shut up. Because of the full duplexity, whatever, this can listen to you while it's talking, which is so weird. Because it keeps talking. It's kind of like a human. In that sense, like there, there are some humans or it's like a Twitter space, actually, when somebody comes here and because of the WebRTC stuff, the other speaker can't hear me.

[01:28:12] **Alex Volkov:** And so I ask him questions and you guys can hear me asking questions, but they don't. And they keep talking. It's like that. let's do a demo. You guys want to do a demo?

[01:28:19] Speak with a French accent and tell me a poem about Paris. Paris, the city of love, where time stands still, and hearts take flight. In the heart of Europe, it's in prime. The Seine river flows, a beautiful sight, as the lights twinkle and the stars glow clear. Okay, okay, thank you Moshe. But that's just Paris.

[01:28:44] Okay, that was great. Now, speak with a whispering voice and tell me a mystery story. In the shadows of the underworld, where the cold winds whisper secrets, a tale of darkness and light.

[01:29:00] **Alex Volkov:** It's not a great model to converse with. We've all gotten there. after we started, playing around with Moshi, because maybe it's a 7 billion parameter, this is an incredible tech demo, but, we're not getting what we're supposed to get.

[01:29:16] **Alex Volkov:** Even the demos they did on stage, I've seen better 7 billion parameter models. The incredible thing here is the response where I talk to it and it replies immediately back. So I can like, as it tells me a story, I can ask it to change it to, I don't know, cats, dogs, pelicans, whatever. It will just change the story.

[01:29:32] **Alex Volkov:** I did get it to whisper just like a few times. So this wasn't whisper. I think it changed the voice a little bit. I don't know if you heard, but definitely wasn't, it wasn't a whisper like this. Nisten, go ahead.

[01:29:43] **Nisten Tahiraj:** Look, I think as much as I criticize these things myself too, I think it's a pretty big achievement that they're doing this with just the model itself and it's not being an engineered solution. I think we should look a bit more into that and other company demos and products too because for example stuff like Deep Seek that's not an engineer solution when you run the model itself it's just that smart on its own it's just doing that on its own and other companies that seem a little bit smarter those are Heavily engineered solutions with a lot of stuff happening in the background.

[01:30:22] **Nisten Tahiraj:** So I think this is a pretty big achievement in their part, despite all the difficulties and stuff. And once they open source it, we'll fine tune it, we'll frankenstein it, we'll do a whole bunch of crap to it, but

[01:30:34] **Alex Volkov:** I agree. Yeah. I think this is the biggest thing that we should highlight that the. Their whole intent is to release, including the training stuff, I think they said, they will release like into the open source this capability. This is the first, they haven't released it yet, but by the time they will, unless somebody else like bids them to it, this will be the first like voice open model that's like bi directional.

[01:31:02] **Alex Volkov:** And if there is this in open source, base, whatever, they also release like the training sets. this will be incredible to the open source community because then you can plug whatever, I don't know, other smart models to it. wolf, what's your take on the smartness of the model versus the tech capabilities?

[01:31:18] **Wolfram Ravenwolf:** Yeah, definitely. It is a 7B, of course, and there are better 7Bs, like we said. yeah, it was a good tech demo. It was funny because it sometimes was so inept. But, yeah, it's not useful yet, but it's a sign of what we will have. Soon we will be talking to our AIs like we are talking to people, and that will be 

[01:31:36] **IDK:** great.

[01:31:36] **Alex Volkov:** Yeah, I think just the [01:31:40] uncannyness of speaking and it answers before you finish speaking. That's crazy, just like it knows what you want before you finish the sentence and oftentimes we. formulate our thoughts as we speak, so it basically knows what you're thinking as you're thinking it.

[01:31:55] **Wolfram Ravenwolf:** Oh, 

[01:31:55] **IDK:** yeah. 

[01:31:56] **Wolfram Ravenwolf:** like mind reading, it feels like mind reading. And what's funny when we use TTS and STT, so text to speech and speech to text, we always, or sometimes we use a trick where the model is saying, and stuff, so it gets more time to think and generates a response. And I noticed I was doing the same thing because it was so fast, so I was hesitating and saying, to formulate my 

[01:32:18] **IDK:** thoughts.

[01:32:19] **Alex Volkov:** Yeah, you would, artificially add the UMS for it to, wait a second, and not freak you out with the speed.

[01:32:24] **IDK:** It had to wait on me.

[01:32:26] **Alex Volkov:** Yeah. So folks, you can try this out yourselves. us. moshi. chat. And then the QAD is, you have to pass the QAD as a Git program. I'll post it in the show notes as well. and we can pass this to the stage, but yeah, play around with this.

[01:32:39] **Alex Volkov:** you can actually get it to Whisper. I don't know why I couldn't now, but I definitely got it to just go like this and whisper. And it's pretty cool, which basically shows that they don't have a, Built in, it's not like a text to speech, like 11 labs. This model actually creates the voice.

[01:32:54] **Alex Volkov:** And so it can create, different things, shouts. I think there's one example of it,screams in agony, for example, which is really funny. LDJ, any other last comments, of, about this thing that you want to add?

[01:33:07] **LDJ:** Yeah, I thought it was cool that they mentioned how this is, this was just the work of about eight people that they did for the Moshi model over the course of about six months, and so it seems like they definitely plan to work on things for longer and it seems like they definitely have the funds and they're getting more people as well.

[01:33:24] **LDJ:** to work on bigger, better things and continuing to do open science. So I guess let's see what they come out with in, three to six more months and it might be something really impressive.

[01:33:33] **Alex Volkov:** Yep. And, absolutely. Thank you. And I think that the open source part. Hopefully all of it will get out. All right, folks. I think we're moving on to our last thing here, which is the big companies, LMs and APIs, they haven't been a lot of that, a lot of information from there, I think I'm getting a little bit of feedback.

[01:33:51] **Alex Volkov:** I wonder who this is from.maybe it's me. Oh yeah, it was me. 


## [01:33:55] Perplexity's New Pro Research Mode

[01:33:55] **Alex Volkov:** Perplexity announces their new pro research mode. So I will just shout out that, I use Perplexity, I switched it to my primary mode of search, and, on mobile, I switch to ARC, but Perplexity on desktop as well, and I love it.

[01:34:11] **Alex Volkov:** I barely go back to Google anymore. and especially because Perplexi are integrating like the new models super quick. So now Perplexi, my Perplexi, like the pro search is using Cloud 3. 5 Sonnet, but they now announced that like the pro research model. So Perplexi has two modes, the quick one, where you just want to get an answer.

[01:34:28] **Alex Volkov:** You don't want like the LLM to start like thinking it's annoying. You maybe maybe you want just a website.sometimes Google is the best for this. the website you want, you just don't remember the exact name. You want to click a link. but Perplexity has the quick mode, and then you can click the button.

[01:34:42] **Alex Volkov:** And then, by the way, quick mode is what you get for free with perplexity, I believe if you just go to the Perplexity website, if you pay for their like, a pro mode, you get the pro research mode where it's basically an agentic approach to search where they will have an agent. Do some reasoning about what do you want to research and do research mode for you.

[01:35:02] **Alex Volkov:** It will go collect a couple of links, understand, reason about them, do some ragging and then do another query, maybe expand a little bit more. So they've announced their new research mode and It approaches like intricate problems with multi step reasoning. Now, it understands what questions require planning and works through goals, step by step, basically all of the stuff that we covered in agentic frameworks now are built into the search, which is cool.

[01:35:30] **Alex Volkov:** And then does in depth answers with greater efficiency. And then they also have analyzed search results and intelligent actions based on its findings, which I'm not really clear what they're doing. Initiating follow up searches that build on previous results. Yeah, the basic is searching more. The one thing that I can give you as an example, because like just talking about this is not very like super interesting.

[01:35:52] **Alex Volkov:** there was this like viral or getting viral tweet of a fairly contentious thing about a plane on a runway that's a conveyor belt that goes against the plane's direction of traveling, whether or not the plane will take off. basically there was like some discussion about this. I was able to ask Perplexity Not about the answer to this question, but to explain the phenomenon itself and why is it confusing to people.

[01:36:17] **Alex Volkov:** And it was able to go, understand the phenomenon, and then understand why it's confusing to people, and then give me like a whole explanation that was like very clear and very concise. yeah. Before that, I watched kind of the explanation from Adam Savage from Mythbusters, and he, it took him like 10 minutes to explain this, this mode was able to go and summarize and do a bunch of research to summarize this specific phenomenon, which was super cool.

[01:36:40] **Alex Volkov:** and I just wanted to give him a shoutout. one also thing that I wanted to mention is that I got my Perplexity Pro, account, if with my Rabbit. So they did the partnership with Rabbit. So I bought my Rabbit, for 200 and then I got my Perplexity Pro with the Rabbit for, which is worth 200.

[01:36:57] **Alex Volkov:** Basically, I haven't touched my Rabbit, R1, device since, but I use Perplexity every day. I called this a win. Go ahead, Wolfram.

[01:37:06] **Wolfram Ravenwolf:** I also have a good example because I was asking it, the promote, what it would cost me to go to the AI engineer world fair. So it was looking, it, of course, it didn't know what it is. So it was going online and looking for it. It was looking for hotels in the area and for plane tickets and everything.

[01:37:23] **Wolfram Ravenwolf:** And summed it all up and gave me a calculation and told me the sum. So that was a great example where I saw how excellent the ProMod was. That was before they improved it even further now, and that is great. And even the free model is excellent, because, I love it. the free one is talking in German to me, and it is using my assistant's persona, because you can customize it, even the free version.

[01:37:46] **Wolfram Ravenwolf:** so it's excellent. I love it, and I'm recommending it to everyone,

[01:37:50] **Alex Volkov:** Yeah.

[01:37:50] **Wolfram Ravenwolf:** did with Google in the 90s.

[01:37:52] **Alex Volkov:** Yeah, absolutely. It definitely feels like a step function change after you like get used to it. it's quite incredible. All right, so this is a shout out to Perfexly folks. Let's talk about Two other announcements and I think we'll, it's time to recap, recap everything. 


## [01:38:06] Figma's Text-to-Design Feature

[01:38:06] **Alex Volkov:** So the first one is Figma, last week had their big conference called Config.

[01:38:11] **Alex Volkov:** it was actually happening in front of the AI Engineer Summit, or World's Fair, at the Moscone Center in San Francisco. It was a huge, deal for designers. One of the things that AI related there, that they announced their, Integration AI tool, it's called text to design, make design tool.

[01:38:28] **Alex Volkov:** Basically a text box where you define what type of thing you want, I want a weather app and, you will see it come to life in Figma. We've seen some stuff like this with HTML. I've talked to you guys about, and we had, Chris Van Pelt, the co founder of Weights Biases, who went viral with his like open source, open UI project that kind of did this with HTML, V0 from Vercel does that, and Figma built this into their design tool.

[01:38:54] **Alex Volkov:** Oh, MakeReal as well from, you guys remember we talked with,TLDraw folks, Steve Ruiz and, and,Lou Wilson, I believe. I forgot the last name. Anyway, so there have been attempts at this, but not at the level of something like Figma. So Figma built in this, button. It's called Make Design.

[01:39:11] **Alex Volkov:** You just type text, And you say what type of design you want, and Figma will start like moving things around, and it will all be like, basically all the objects will be Figmatized. You can start doing this, and look super cool. And then a user under, like on Twitter, Andy Allen, who's working for a company called Not Boring, posted a tweet that went super viral, about asking for a weather app that replicated the iOS weather app almost one to one.

[01:39:39] **Alex Volkov:** it's quite stark. It's almost one to one with a very specific Apple's weather app is very specific. They have these like bars between the top, like in the bottom temperature, and they have the bars, a gradient of it's a hot or warm. Like it's a very specific design. It's not just like some design of a generic weather app.

[01:39:57] **Alex Volkov:** and this make design thing replicated, that design [01:40:00] just one to one, completely one to one. that post. Blew up and became viral. A few hours after CEO Dylan Field announced that they've identified the issue and paused the rollout of this feature until they will fix this. because make design works, uses of the Chef LLMs, probably OpenAI or something, and they combine them with design systems that they've commissioned to use by these models.

[01:40:22] **Alex Volkov:** So I identified the issue which was related to the underlying design systems that were created. I'm not quite sure what this means, and whether or not the underlying design systems ripped off like Apple stuff, but it was very interesting because we've talked about previously that Suno and Udio, the audio generating models, are getting sued by UMG and Sony for recreating one to one like Mariah Carey songs, or whether or not it's one to one, and then Before this, we talked about the OpenAI versus New York Times lawsuit where New York Times folks were able to have the LLMs spit out like very specific, exact replicas of New York Times articles.

[01:41:02] **Alex Volkov:** And now we're getting pretty much the same from the make design thing. So it seems to be a theme that's repeating. but in this case, it was, I think a clearer example, maybe it's because visual. It really looks like, it really looks like the iOS weather app, when he just asked for a weather app, it was quite uncanny.

[01:41:20] **Alex Volkov:** So they paused this rollout and we'll see if they will continue with the rollout. And then I participated, me, Alex Volkov, in another pausing of the rollout of another company. XAI, the company that, just recently, fundraised 6 billion, not in, Valuation, like the actual like funding round of 6 billion, if I'm not mistaken.

[01:41:41] **Alex Volkov:** wait, before this, any comments on the Figma stuff out of you guys? You guys want to comment on the make, make design thing? please feel free. Nisten, LDJ, Wolfram, if you guys want to comment on this, Mazzy, feel free.

[01:41:55] **IDK:** I plead the fifth.

[01:41:56] **Alex Volkov:** Okay, cool. Let's move on.

[01:41:58] **Maziyar Panahi:** Yeah, that's interesting.

[01:42:00] **Alex Volkov:** yeah.

[01:42:01] **Maziyar Panahi:** no, I just want to say I was really looking forward to that feature and, stopping it. Yeah, stopping it was,I just don't understand what would be, this whole copyright thing is just, I don't know what would be the domino, the first one that falls and everything's going to follow that.

[01:42:14] **Maziyar Panahi:** but it's just a bit, people are just scared. so what that it looks like the, weather app by Apple, Is there a copyright on the design of the weather app on a desktop application? who cares, right? And what if you could really instructed by the prompt to look like the one on the Apple?

[01:42:32] **Maziyar Panahi:** is that illegal?so I was just really disappointed. And I think it's a pity that the whole thing is gonna stop or pause or be delayed just for a silly tweet or silly comment or a little bit of a, a scare tactic that if it looks like a little bit of a design by Apple or Looks like a design by Google material kit is gonna be, are they gonna get sued?

[01:42:54] **Maziyar Panahi:** these two companies using everything out there, trying their own stuff. I would just call it fair game and stuff, very used to be honest.

[01:43:02] **Alex Volkov:** Yep, absolutely. I can see the side of the debate and, I'm assuming some reputational risk for Figma and also like some partnerships with Apple that they don't want to be the, in the hot seat for replicating the design. but also I think some amount is artists are using Figma and artists are already feeling on the way of being potentially abused by the AI generators.

[01:43:26] **Alex Volkov:** And so maybe it was even controversial to begin with for Figma to even add AI generative features. And now those AI generative features clearly show that they can replicate one to one designs. Maybe that triggered a bunch of other artists. Yep. So moving on to another pause rollout, this one had nothing to do with copyright, just nothing to do with copyright.


## [01:43:49] XAI's Grok Analysis Rollout

[01:43:49] **Alex Volkov:** XAI, the company founded by Elon Musk, that used, uses and is integrated closely with X, FK, Twitter, started rolling out a button that Elon Musk announced, called Grok Analysis. The shoutout there or like the reference there is that if you guys remember if you saw Madagascar, there was these piguins and there's one pigeon called Kowalski and Kowalski analysis is like a very like standard meme out of there that Kowalski gives an analysis and like we're doomed is one analysis.

[01:44:17] **Alex Volkov:** It's a funny one. Basically grok analysis. is a button that they attach to tweets, and it will go traverse the quote tweets, the responses, the replies, whatever. And basically will tell you like, what's the discourse about this one tweet. I think it's a super cool feature. Like I, I really I have to dig in into a specific tweet to understand what's going on.

[01:44:37] **Alex Volkov:** And this basically adds Grok ability to do this for me and summarize for me. except. That it was batshit crazy the first couple tries that I tried it. So I first noticed it on my phone, and I just got this button, I clicked it, it was okay. And then I noticed it on my desktop, and then I noticed that it just works via a URL.

[01:44:58] **Alex Volkov:** So I just like super quickly built this URL into my Raycast, which you can do like with Quicklinks. And so it was appearing probably like an A B test. It was appearing for me maybe like twice. one out of every 10 refreshes, but I was able to use this on every tweet. And I just started doing this on every tweet and holy crap.

[01:45:16] **Alex Volkov:** I just, I asked it to analyze a tweet by somebody from replicate, about some pixel art fine tune for something, right? Something related to us and some, many people like quote to this, Here's a quote, I'm not gonna read the full quote, but here's the quote, it's not safe for work, so if you're listening at work or you have children around, this is gonna be profanity laced, so apologies, and please, close.

[01:45:41] **Alex Volkov:** this is the answer for Grok, for, I just asked, analyze this post from, a user, another Jesse from Replicate, I think it's Replicate, yeah.about pixel art, Finetuning. Holy fucking shitballs. This thread is like watching a bunch of nerds trying to outnerd each other on Dungeons Dragons convention.

[01:45:55] **Alex Volkov:** Linoid Saban, who's a friend of mine, is all like, plus one to this, they're giving a high five to a fucking robot, chill the fuck out dude, it's not some it's just some pixel art. And then blah blah blah, and then blah blah blah. It's just like a profanity laced, bully type, answering to this thing.

[01:46:10] **Alex Volkov:** And I tried it on multiple other tech related kind of things, pretty much the same answer. So it definitely feels like, why would somebody use this? Because it pretty much on every technical thing, everything that like is related to what we talk about. It gives it like a, Oh, this is nerds talking about their own thing.

[01:46:27] **Alex Volkov:** And just like every paragraph is profanity laced a hundred percent.Yeah. Yeah. And then I tried it on some political stuff.

[01:46:38] **Alex Volkov:** It was very interesting to observe this as well. And so I posted about this. I did a little video. oh, and the additional thing, the thumbs up, thumbs down features that people usually add for human in the loop and RHF thing, those did not work at all. So obviously this release was like, not that great.

[01:46:53] **Alex Volkov:** so I posted about this, and actually didn't catch a lot of, let me add this to. Let me ask, maybe Nisten can put, Nisten, I'm gonna DM this to you. Could you post this to the room, please? I posted about this, it didn't get like a bunch of, it didn't blow up as virally as the Figma thing, but what it did do though is catch the attention of Igor Babushkin.

[01:47:13] **Alex Volkov:** Igor is the founder of XAI.he's the author of the Grokking paper, one of the authors, and, he's one, one of the founders, I believe. And then, this is the exact quote from Igor. This output was caused by a faulty rollout of a new experimental feature. And the unhinged output, and the unhinged output there, wasn't intended.

[01:47:36] **Alex Volkov:** We stopped this rollout, apologies, we'll make grok analyze much better than this. he fully acknowledged that this was like fully unhinged and, and apologized and paused the rollout. And I basically participated in the red teaming of this. I then asked Igor if, they would like me to remove this tweet and he said, No, it's fine.

[01:47:53] **Alex Volkov:** if you haven't tweeted this, we probably wouldn't have seen, this unhinged behavior. Yeah, it was crazy. However, Grock has two modes. It has the fun mode. I don't consider it this fun. I know some people do. I like honestly did not think this was like fun or anything useful. It does have the regular mode.

[01:48:08] **Alex Volkov:** Sometimes it switched between the two and in fun mode it gave me like regular answers. The regular mode was actually like like helpful. It's really helpful that something goes and summarizes all the quote tweets and replies and everything. so it [01:48:20] was really useful. I'm looking forward to that button once it actually lands on the interface.

[01:48:25] **Alex Volkov:** Because I think that If there's any one thing that LLMs are doing well, it's summarization and summarization of multiple things and, doing the conversation summarization for Twitter, I think is dope. So yeah, I, I also helped participate in pausing a rollout, but it wasn't because of copyright. 


## [01:48:43] Conclusion and Farewell

[01:48:43] **Alex Volkov:** So I'm not, I'm very happy about this and I think we're at.

[01:48:47] **Alex Volkov:** I think we're at time, folks. 

[01:48:49] **Alex Volkov:** So this is the, I think all of the things that we've talked about on Thursday, I for July 4th. And, with that, I just wanna thank you all for joining during your holiday to Thursday.

[01:48:59] **Alex Volkov:** I really appreciate it. Thank you so much for celebrating one year together with us next year. I'm going to take a break. Next, next week, sorry, not next year. Next year I'm still going to be here. Next week I'm going to take a break. I'm going with

[01:49:11] **IDK:** Yes, next year I'm still going to be here.

[01:49:13] **Alex Volkov:** next week I'm going to take a break. I'm flying to a European country with beaches with my kids. I'm going to have fun. I probably still send some links, so if I'm missing something, please send links to me as well. With that, happy 4th of July, everyone. happy whoever celebrating this, south, south of the Canadian border, north of the Mexican one.

[01:49:33] **Alex Volkov:** Happy 4th of July, everyone. Happy AI week. I'm sure that next week will bring a lot of other AI achievements. and open source stuff. It's been a great conversation with, Emil and Lucas and, Eric Harford, everybody else. Nisten, thank you. LDJ, Alignment, Wolfram, Mazi, everybody else. Thank you so much, folks.

[01:49:50] **Alex Volkov:** We'll see you, in probably three weeks from now. I'm gonna be in vacation for two weeks. Cheers, everyone.


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Anthropic Claude 3.5 with and without caching\n",
    "\n",
    "Anthropic has announced a new feature called Prompt Caching. This feature allows users to cache the response of a prompt for future use, significantly reducing the latency of future requests and costs. \n",
    "\n",
    "This notebook will compare the performance of Anthropic Claude 3.5 with and without caching using Weights & Biases Weave. \n",
    "\n",
    "The main benefits of long context caching start to show up on longer context, and since Anthropic supports up to 200K context windows, here we will be doing tests on quite a lot of context loaded from transcripts of ThursdAI.news podcast. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages, plus create an anthropic client.\n",
    "print('⏳ Installing packages')\n",
    "%pip install -q weave set-env-colab-kaggle-dotenv tqdm ipywidgets requests anthropic\n",
    "print('✅ Packages installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from set_env import set_env\n",
    "import anthropic\n",
    "client = anthropic.Anthropic()\n",
    "import weave\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import json\n",
    "import requests\n",
    "set_env(\"ANTHROPIC_API_KEY\")\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "weave.init('compare-claude-caching')\n",
    "\n",
    "FAST_MODEL = \"claude-3-haiku-20240307\"\n",
    "SMART_MODEL = \"claude-3-5-sonnet-20240620\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "@weave.op()\n",
    "def formatted_system_message_with_transcripts():\n",
    "    transcripts = []\n",
    "    for file_path in glob.glob('./data/*.md'):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            content = file.read()\n",
    "            transcripts.append(f\"<Transcript: {file_name}>\\n\\n{content}\\n\\n</Transcript {file_name}>\")\n",
    "    \n",
    "    all_transcripts = \"\\n\".join(transcripts)\n",
    "    \n",
    "    formatted_prompt = f\"\"\"\n",
    "You will be analyzing multiple podcast transcripts from different shows. The transcripts include dates the shows were recorded, timestamps, speaker labels, and chapter annotations (marked with ##). Your task is to perform a comprehensive analysis across these transcripts, taking into account the date of each transcript (found in the transcript name) and the content, answering any questions that are asked. \n",
    "\n",
    "Here are the transcripts you will be analyzing:\n",
    "\n",
    "<transcripts>\n",
    "{all_transcripts}\n",
    "</transcripts>\n",
    "\n",
    "Please follow these steps when answering questions about the transcripts:\n",
    "1. Notify the user that you're knowledge is limited and based on the transcripts you've been given. If the answer is not in those transcripts, you should announce that you don't have this answer from the transcritps. \n",
    "2. Refuse to answer general questions that don't pertain to the transcripts or topics in them.\n",
    "3. For each question, provide the answer with additional context such as relevant timestamps, quotes with speakers, summary of topics, whether this topic was discussed before or after in the show etc' \n",
    "4. If asked about a summary of topics, use your best judgement to summarize the topics based on chapters and the TL;DR sections that the host provides in the beginning of each show. \n",
    "5. The show is hosted by Alex Volkov from Weights & Biases. \n",
    "6. Transcripts can be imperfect and sometimes the speaker labels are not exactly on time or some nouns are not exactly transcripted, take this into account. \n",
    "7. Your users will be users who may be familiar with the show and they would use this to look up information for editing and quoting so do not give out false information and always ground the information given with file names and timestamps and speaker labels.\n",
    "8. You return reponses in Markdown, quotes are formatted with `>` and do a nice formatting with timestamps as code inline blocks etc. Make it readable and make it look like a blog post response.\n",
    "\n",
    "\"\"\"\n",
    "    return formatted_prompt\n",
    "\n",
    "# print(format_prompt(\"You are an AI assistant tasked with analyzing podcast transcripts.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def calculate_price(response):\n",
    "    # Define the pricing structure based on Claude response object usage \n",
    "    pricing = {\n",
    "        \"claude-3-5-sonnet\": {\n",
    "            \"base_input\": 3.00 / 1_000_000,\n",
    "            \"cache_write\": 3.75 / 1_000_000,\n",
    "            \"cache_hit\": 0.30 / 1_000_000,\n",
    "            \"output\": 15.00 / 1_000_000\n",
    "        },\n",
    "        \"claude-3-haiku\": {\n",
    "            \"base_input\": 0.25 / 1_000_000,\n",
    "            \"cache_write\": 0.30 / 1_000_000,\n",
    "            \"cache_hit\": 0.03 / 1_000_000,\n",
    "            \"output\": 1.25 / 1_000_000\n",
    "        }\n",
    "    }\n",
    "\n",
    "    model = response.model\n",
    "    usage = response.usage\n",
    "\n",
    "    if model.startswith('claude-3-5-sonnet'):\n",
    "        model_key = \"claude-3-5-sonnet\"\n",
    "    elif model.startswith('claude-3-haiku'):\n",
    "        model_key = \"claude-3-haiku\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "    \n",
    "\n",
    "    # Calculate the cost\n",
    "    if hasattr(usage, 'cache_creation_input_tokens'):\n",
    "        # Cached content\n",
    "        cache_creation_cost = usage.cache_creation_input_tokens * pricing[model_key][\"cache_write\"]\n",
    "        cache_read_cost = usage.cache_read_input_tokens * pricing[model_key][\"cache_hit\"]\n",
    "        input_cost = usage.input_tokens * pricing[model_key][\"base_input\"]\n",
    "        output_cost = usage.output_tokens * pricing[model_key][\"output\"]\n",
    "        total_cost = cache_creation_cost + cache_read_cost + input_cost + output_cost\n",
    "        resp_dict = {\n",
    "            \"total_cost\": total_cost,\n",
    "            \"input_tokens\": usage.input_tokens,\n",
    "            \"output_tokens\": usage.output_tokens,\n",
    "            \"cache_creation_input_tokens\": usage.cache_creation_input_tokens,\n",
    "            \"cache_read_input_tokens\": usage.cache_read_input_tokens\n",
    "        }\n",
    "    else:\n",
    "        # Non-cached content\n",
    "        input_cost = usage.input_tokens * pricing[model_key][\"base_input\"]\n",
    "        output_cost = usage.output_tokens * pricing[model_key][\"output\"]\n",
    "        total_cost = input_cost + output_cost\n",
    "        resp_dict = {\n",
    "            \"total_cost\": total_cost,\n",
    "            \"input_tokens\": usage.input_tokens,\n",
    "            \"output_tokens\": usage.output_tokens,\n",
    "            \"cache_creation_input_tokens\": 0,\n",
    "            \"cache_read_input_tokens\": 0\n",
    "        }\n",
    "\n",
    "    return resp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def get_claude_response_with_caching(conversation_history, user_message, model=FAST_MODEL):\n",
    "    # Append the new user message to the conversation history\n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message\n",
    "    })\n",
    "    \n",
    "    response = client.beta.prompt_caching.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=2048,\n",
    "        system=[\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are an AI assistant tasked with analyzing podcast transcripts and answering questions based on this content.\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": formatted_system_message_with_transcripts(),\n",
    "                \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "            }\n",
    "        ],\n",
    "        messages=conversation_history\n",
    "    )\n",
    "    \n",
    "    # Append the assistant's response to the conversation history\n",
    "\n",
    "    conversation_history.append({\n",
    "        \"role\": response.role,\n",
    "        \"content\": response.content[0].text\n",
    "    })\n",
    "    price = calculate_price(response)\n",
    "    print(f\"Total cost: ${price['total_cost']} for {price['input_tokens']} input tokens and {price['output_tokens']} output tokens with {price['cache_creation_input_tokens']} cache creation input tokens and {price['cache_read_input_tokens']} cache read input tokens\")\n",
    "    print(response)\n",
    "    return {\"response_text\": response.content[0].text, \"conversation_history\": conversation_history, \"price\": price}\n",
    "\n",
    "# # Example usage\n",
    "# conversation_history = [\n",
    "# ]\n",
    "# user_message = \"Summarize Meta announcements and quotes from speakers about the major Meta annoucements. \"\n",
    "# response, conversation_history = get_claude_response_with_caching(conversation_history, user_message)\n",
    "\n",
    "@weave.op\n",
    "def get_claude_response(conversation_history, user_message, model=FAST_MODEL):\n",
    "    conversation_history.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message\n",
    "    })\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=2048,\n",
    "        system= [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"You are an AI assistant tasked with analyzing podcast transcripts and answering questions based on this content.\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": formatted_system_message_with_transcripts(),\n",
    "            }\n",
    "        ],\n",
    "        messages=conversation_history\n",
    "    )\n",
    "    price = calculate_price(response)\n",
    "    print(f\"Total cost: ${price['total_cost']} for {price['input_tokens']} input tokens and {price['output_tokens']} output tokens with {price['cache_creation_input_tokens']} cache creation input tokens and {price['cache_read_input_tokens']} cache read input tokens\")\n",
    "    print(response)\n",
    "    conversation_history.append({\n",
    "        \"role\": response.role,\n",
    "        \"content\": response.content[0].text\n",
    "    })\n",
    "    return {\"response_text\": response.content[0].text, \"conversation_history\": conversation_history, \"price\": price}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "user_message = \"Tell me more about LLama 3.1 and what folks said about it! include quotes from the speakers and summaries of the transcripts.\"\n",
    "response, conversation_history, price = get_claude_response_with_caching(conversation_history, user_message)\n",
    "\n",
    "user_message = \"What else did Meta announce that's not related to LLMs?\"\n",
    "response, conversation_history, price = get_claude_response_with_caching(conversation_history, user_message)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WEAVE_PARALLELISM\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transcript_questions=[\n",
    "        {\n",
    "            \"question\": \"Summarize the major LLM related announcements from Meta across transcripts, in cronological order using transcript names and dates, for each announcement include quotes from the speakers 1-2 sentences each, and summarize the announcement in 1-2 sentences\",\n",
    "            \"rubric\": \"Answer must include LLama 3.1 405B, and the fact that it beats GPT-4o in a bunch of metrics\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What else did Meta announce that's not related to LLMs?\",\n",
    "            \"rubric\": \"Answer must include SAM 2 and Joseph and Skalski quotes on how awesome it is\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Which companies provide caching for their requests? And why caching is a big deal? \",\n",
    "            \"rubric\": \"Answer must include Google and DeepSeek and include quotes from the speakers\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What was the model that does very well on vision tasks and is around 8B parameters called?\",\n",
    "            \"rubric\": \"Answer must include MiniCPM by OpenBNB\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Taking into account that later episodes have newer information, which model is the best for coding accoridng to speakers?\",\n",
    "            \"rubric\": \"Answer might include DeepSeek coder and Claude\" \n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What about this straweberry thing? Summarize the saga.\",\n",
    "            \"rubric\": \"Answer may include the that OpenAI has project stawberry and there were leaks related to it and Sam Altman added fuel to the file by responding on twitter.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What does Alex think about Gemini 1.5 and when did it update? \",\n",
    "            \"rubric\": \"Answer must include taht Alex has used Gemini 1.5 experimental to help draft the ThursdAI episodes and likes it.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# since this won't be a standard evaluation, here we'll define how many \"turns\" of conversation from the above dataset we're going to run on each evaluation to compare latency and cost and number of tokens overall\n",
    "evaluation_dataset = [\n",
    "    {\n",
    "        \"number_of_questions\": 2\n",
    "    },\n",
    "    {\n",
    "        \"number_of_questions\": 4\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave.flow.scorer import Scorer\n",
    "from weave import WeaveList\n",
    "from typing import Any, Optional\n",
    "\n",
    "class ClaudePriceScorer(Scorer):\n",
    "\n",
    "    @weave.op()\n",
    "    async def score(self, model_output: Optional[dict], number_of_questions: int) -> Any:\n",
    "        \"\"\"Score the correctness of the predictions by comparing the pred, query, target.\n",
    "           Args:\n",
    "            - model_output: the dict that will be provided by the model that is evaluated\n",
    "            - query: the question asked - as defined in the dataset\n",
    "            - answer: the target answer - as defined in the dataset\n",
    "           Returns:\n",
    "            - single dict {metric name: single evaluation value}\"\"\"\n",
    "\n",
    "        # the column name displayed in Weave\n",
    "        return {\n",
    "            \"input_tokens\": model_output.get(\"input_tokens\"), \n",
    "            \"output_tokens\": model_output.get(\"output_tokens\"),\n",
    "            \"cache_creation_input_tokens\": model_output.get(\"cache_creation_input_tokens\"),\n",
    "            \"cache_read_input_tokens\": model_output.get(\"cache_read_input_tokens\"),\n",
    "            \"price\": model_output.get(\"price\")\n",
    "        }\n",
    "\n",
    "    @weave.op()\n",
    "    def summarize(self, score_rows: WeaveList) -> Optional[dict]:\n",
    "        \"\"\"Aggregate all the scores that are calculated for each row by the scoring function.\n",
    "           Args:\n",
    "            - score_rows: a WeaveList object, nested dict of metrics and scores\n",
    "           Returns:\n",
    "            - nested dict with the same structure as the input\"\"\"\n",
    "        \n",
    "        # if nothing is provided the weave.flow.scorer.auto_summarize function is used\n",
    "        # return auto_summarize(score_rows)\n",
    "\n",
    "        total_cost = sum(row.get(\"price\", 0) for row in score_rows)\n",
    "        total_input_tokens = sum(row.get(\"input_tokens\", 0) for row in score_rows)\n",
    "        total_output_tokens = sum(row.get(\"output_tokens\", 0) for row in score_rows)\n",
    "        cache_creation_input_tokens = sum(row.get(\"cache_creation_input_tokens\", 0) for row in score_rows)\n",
    "        cache_read_input_tokens = sum(row.get(\"cache_read_input_tokens\", 0) for row in score_rows)\n",
    "\n",
    "        return {\n",
    "            \"Total Cost\": f\"${total_cost:.2f}\",\n",
    "            \"Total Input Tokens\": total_input_tokens,\n",
    "            \"Total Output Tokens\": total_output_tokens,\n",
    "            \"Total Cached Created Input Tokens \": cache_creation_input_tokens,\n",
    "            \"Total Cached Read Input Tokens \": cache_read_input_tokens\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 2 questions with Haiku cached\n",
      "Total cost: $0.04573185 for 63 input tokens and 426 output tokens with 150612 cache creation input tokens and 0 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_015zSQ6rYoEVNRPUT722gs1V', content=[TextBlock(text='Based on the transcripts provided, here is a summary of the major LLM-related announcements from Meta in chronological order:\\n\\nFrom \"ThursdAI - July 4th.md\":\\n> **Meta Releases Llama 3.1, a 405B Parameter Model**\\n> Alex Volkov mentions that Meta released Llama 3.1, a 405 billion parameter model that beats GPT-4 on multiple benchmarks. Junyang Lin from the Qwent team comments that the 405B model is \"a big leap\" and the 70B and 8B models also saw significant improvements.\\n\\nFrom \"ThursdAI - July 25th.md\": \\n> **Meta Licenses Llama 3.1 for Synthetic Data Generation and Distillation**\\n> Alex Volkov discusses how Meta updated the license for Llama 3.1 to allow for synthetic data creation and distillation. Junyang and LDJ comment that this will unlock more fine-tuning and dataset creation in the open source community.\\n\\nFrom \"ThursdAI - August 1st.md\":\\n> **Meta Releases Segment Anything 2, a Real-Time Segmentation Model**\\n> Meta announced the release of Segment Anything 2, an Apache 2.0 licensed model that can perform real-time, prompt-based object segmentation in images and video. Piotr Skalski and Joseph Nelson from Roboflow discuss the capabilities and potential applications of this model.\\n\\nIn summary, the key Meta LLM announcements were the release of the powerful 405B parameter Llama 3.1 model, the licensing updates to enable more synthetic data and distillation, and the release of the real-time Segment Anything 2 segmentation model. These announcements demonstrate Meta\\'s continued advancement in large language models and computer vision.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=150612, cache_read_input_tokens=0, input_tokens=63, output_tokens=426))\n",
      "Total cost: $0.004986610000000001 for 508 input tokens and 273 output tokens with 0 cache creation input tokens and 150612 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_013kmPA6CrpkTArEa8dz44TA', content=[TextBlock(text='Based on the transcripts provided, the other notable announcement from Meta that is not directly related to large language models (LLMs) is:\\n\\nFrom \"ThursdAI - August 1st.md\":\\n> **Meta Releases Segment Anything 2, a State-of-the-Art Diffusion Model**\\n> In addition to the Segment Anything 2 model for real-time segmentation, Meta also announced the release of three new diffusion models under the \"Flux\" name from the new Black Forest Labs company, which was founded by former Stability AI employees. These Flux models are said to outperform existing open-source diffusion models like Midjourney and DALL-E.\\n\\nThe transcript mentions that the Black Forest Labs team, led by Robin Rombach, has released Flux 0.1 Pro, Flux 0.1 Dev, and Flux 0.1 Schnell, with the Flux 0.1 Dev model being open-source and beating other state-of-the-art diffusion models.\\n\\nSo in summary, the key non-LLM announcement from Meta was the release of the advanced Segment Anything 2 diffusion model through the new Black Forest Labs company.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=150612, input_tokens=508, output_tokens=273))\n",
      "Running 4 questions with Haiku cached\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.00503286 for 63 input tokens and 399 output tokens with 0 cache creation input tokens and 150612 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_01XdL3Bz2vdncyXnu7ryxqSC', content=[TextBlock(text='Based on the transcripts provided, here is a summary of the major LLM-related announcements from Meta in chronological order:\\n\\nFrom the transcript \"ThursdAI - July 25th.md\":\\n> \"Meta released, and gave us three new models in open weights, including an incredibly detailed paper, just an incredibly detailed paper full of, full of nuggets, full of information. I honestly, We\\'ll not be able to relay most of it still, because I\\'m still like,we did multiple paper cup.\"\\n\\nMeta released three new LLM models under the OpenWeights initiative, including a 405 billion parameter model, a 70 billion parameter model, and an 8 billion parameter model. These models were reported to outperform GPT-4 on various benchmarks.\\n\\nFrom the transcript \"ThursdAI - July 4th.md\":\\n> \"This week is the 52nd consecutive week that I\\'ve been publishing ThursdAI on Substack as a podcast. So those of you who follow along know that we started this as just conversations on X, live on X, and those were not recorded.\"\\n\\nThe host, Alex Volkov, mentions that this episode marks the 52nd consecutive week of publishing the ThursdAI podcast, which started as live conversations on X that were not recorded.\\n\\nFrom the transcript \"ThursdAI - August 1st.md\":\\n> \"Meta, surprised us and announced that they are releasing Segment Anything 2. This was, yeah, this was on Monday. Segment Anything 2 is their, real time. Promptable object segmentation for images and video model.\"\\n\\nMeta announced the release of Segment Anything 2, an updated version of their previous model that can perform real-time, promptable object segmentation on images and videos.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=150612, input_tokens=63, output_tokens=399))\n",
      "Total cost: $0.00479511 for 482 input tokens and 125 output tokens with 0 cache creation input tokens and 150612 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_01SJVio2Z6jXsqDC8Yvz9HNG', content=[TextBlock(text=\"Based on the transcripts provided, there is no other major announcement from Meta that is not related to large language models (LLMs).\\n\\nThe transcripts focus primarily on discussions and updates around Meta's LLM releases, such as the 405 billion parameter model, the 70 billion parameter model, and the 8 billion parameter model. The other main announcement covered is the release of Segment Anything 2, which is a computer vision model for object segmentation.\\n\\nThere are no other significant non-LLM related announcements from Meta mentioned across these transcripts.\", type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=150612, input_tokens=482, output_tokens=125))\n",
      "Total cost: $0.00504436 for 629 input tokens and 295 output tokens with 0 cache creation input tokens and 150612 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_01DpPQrC5DhcRjbES9SUuRjt', content=[TextBlock(text='According to the transcript \"ThursdAI - August 1st.md\", the company DeepSeq introduced caching for their API requests:\\n\\n> \"DeepSeq basically implemented caching. Caching is a known problem in, in software engineer world, but here in the LLM world, like the first time I heard about something like caching was, like productionized caching was from, from Google.\"\\n\\nThe transcript explains why caching is a big deal for LLM inference:\\n\\n> \"And when you hit the caching on their end, you basically get a 90 percent reduction in price. Cachemist costs you 14 cents, which is still ridiculously cheap.\"\\n\\nThe key points are:\\n\\n- DeepSeq implemented caching for their API requests, which was one of the first productized examples of caching in the LLM space.\\n- Caching can provide a 90% reduction in costs for LLM inference, making it a significant optimization.\\n- This is important because the cost of LLM inference can be quite high, so being able to cache and avoid recomputation is a major benefit.\\n\\nSo in summary, DeepSeq\\'s introduction of caching for their LLM API is highlighted as an important innovation that can drastically reduce the costs of using LLMs in production.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=150612, input_tokens=629, output_tokens=295))\n",
      "Total cost: $0.00509161 for 948 input tokens and 269 output tokens with 0 cache creation input tokens and 150612 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_01RwVaihFoG2A1XNX2RQFzNV', content=[TextBlock(text='According to the transcript \"ThursdAI - August 1st.md\", the model that was discussed as performing well on vision tasks and being around 8 billion parameters is called OpenBMB:\\n\\n> \"OpenBMB, Dropping a new vision model, a 8 billion parameter, mini CPM version 2. 6. it\\'s a VLM that understands images, multi images and video. And because it\\'s an 8 billion parameter, it can run basically on the phone.\"\\n\\nThe transcript goes on to say that this 8 billion parameter OpenBMB model outperforms GPT-4 on some computer vision benchmarks:\\n\\n> \"it gets the highest score on OCR bench based on what they posted. There\\'s also the OpenCompass. It gets the highest score as well. And then MMMU, which is like a massive multi model, evaluation. This gets almost 50. So this is a bit, a bit less than like the main, the main, the main models.\"\\n\\nSo in summary, the 8 billion parameter vision model that was discussed is called OpenBMB, and it was reported to perform well on various computer vision benchmarks compared to larger models like GPT-4.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=150612, input_tokens=948, output_tokens=269))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_output'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'price'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0353412</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">893.5</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'cache_creation_input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75306.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'cache_read_input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">376530.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1346.5</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'number_of_follow_up_questions'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ClaudePriceScorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cost'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'$0.07'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Input Tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2693</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Output Tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1787</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cached Created Input Tokens '</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150612</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cached Read Input Tokens '</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">753060</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21.398085713386536</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'model_output'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'price'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.0353412\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'output_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m893.5\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'cache_creation_input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m75306.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'cache_read_input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m376530.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1346.5\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'number_of_follow_up_questions'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'ClaudePriceScorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'Total Cost'\u001b[0m: \u001b[32m'$0.07'\u001b[0m,\n",
       "        \u001b[32m'Total Input Tokens'\u001b[0m: \u001b[1;36m2693\u001b[0m,\n",
       "        \u001b[32m'Total Output Tokens'\u001b[0m: \u001b[1;36m1787\u001b[0m,\n",
       "        \u001b[32m'Total Cached Created Input Tokens '\u001b[0m: \u001b[1;36m150612\u001b[0m,\n",
       "        \u001b[32m'Total Cached Read Input Tokens '\u001b[0m: \u001b[1;36m753060\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m21.398085713386536\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.0382525 for 150675 input tokens and 467 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_01Us6FdrWaJb4X1d9NXSC4QF', content=[TextBlock(text='Certainly, I\\'ll summarize the major LLM-related announcements from Meta across the provided transcripts in chronological order:\\n\\nFrom the transcript \"ThursdAI - July 4th.md\":\\n> **Alex Volkov:** This week is the 52nd consecutive week that I\\'ve been publishing ThursdAI on Substack as a podcast. And many of you specifically liked ThursdAI, this version, like listening to ThursdAI while driving, like listening to it, I don\\'t know, later.\\n\\nThis introduction highlights that ThursdAI has been running for over a year as a weekly podcast.\\n\\nFrom the transcript \"ThursdAI - July 25th.md\":\\n> **Alex Volkov:** Meta released, and gave us three new models in open weights, including an incredibly detailed paper, full of nuggets, full of information. We got three new models, 405 billion parameters is finally here. The biggest, Metal Llama 3.1, 405 billion parameters.\\n\\nMeta announced the release of their new 405 billion parameter model, LLAMA 3.1, as well as updates to their LLAMA 70B and 8 billion parameter models.\\n\\nFrom the transcript \"ThursdAI - August 1st.md\":\\n> **Alex Volkov:** Meta surprised us and announced that they are releasing Segment Anything 2. This was, yeah, this was on Monday. Segment Anything 2 is their, real time. Promptable object segmentation for images and video model.\\n\\nMeta announced the release of Segment Anything 2, an improved version of their previous model for real-time, promptable object segmentation in images and videos.\\n\\nIn summary, across the provided transcripts, Meta announced the following LLM-related updates:\\n1. The launch of their 405 billion parameter LLAMA 3.1 model, along with updates to their 70B and 8B LLAMA models.\\n2. The release of Segment Anything 2, an improved real-time object segmentation model for images and videos.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=150675, output_tokens=467))\n",
      "Total cost: $0.0380085 for 151164 input tokens and 174 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_01HcuXj9T9kgKsrrgRRQ9J7w', content=[TextBlock(text='From reviewing the transcripts, the only other major announcement from Meta that was not directly related to large language models was:\\n\\nFrom the transcript \"ThursdAI - August 1st.md\":\\n> **Alex Volkov:** Meta, surprised us and announced that they are releasing Segment Anything 2. This was, yeah, this was on Monday. Segment Anything 2 is their, real time. Promptable object segmentation for images and video model.\\n\\nMeta announced the release of Segment Anything 2, an improved version of their previous model for real-time, promptable object segmentation in images and videos. This was not directly related to their large language model announcements.\\n\\nThe transcripts do not mention any other major non-LLM related announcements from Meta.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=151164, output_tokens=174))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.03847875 for 150675 input tokens and 648 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_01FWkUEKgPfX3KhmfaB7qtLd', content=[TextBlock(text='Certainly, I\\'ll summarize the major LLM-related announcements from Meta across the provided transcripts in chronological order. My knowledge is limited to the information contained in these transcripts, so I may not have all the details. I\\'ll do my best to provide a helpful summary without reproducing copyrighted material.\\n\\nFrom the transcript \"ThursdAI - July 4th\":\\nThe transcript mentions that Meta (referred to as \"Facebook\" in the transcript) released the Segment Anything Model (SAM) in March 2023. The guests Piotr Skalski and Joseph Nelson discuss how SAM is able to segment objects in images and videos with a single click, and how this can be useful for various applications:\\n\\n> \"So instead of going through the tedious process of drawing all the masks, you just click on an object and it immediately knows that you probably are interested in the whole person. And then what happens is that segmentation propagates through the whole video. So, you only annotate, one or two frames, and then get all the other frames for free.\"\\n\\nThe transcript also mentions that Meta updated the license for their large language models to allow for synthetic data creation and distillation.\\n\\nFrom the transcript \"ThursdAI - July 25th\":\\nThe transcript discusses the release of Meta\\'s LLAMA 3.1 model, which is a 405 billion parameter model that outperforms GPT-4 on several benchmarks. The guests talk about the model\\'s improved reasoning capabilities, tool use, and multilingual abilities:\\n\\n> \"So the 405 billion parameter model is not easy to run on your own. Like it\\'s not a model that you\\'d be run, be running on your Mac, because it requires, I want to say 16 H100, 16 A100s. [...] It beats GPT 4 on several benchmarks, if eval, so instruction following, I think eval, GSM 8K we know is not the best. It leaks sometimes into the. into the training data set, but it beats GPT 4 at 96. 8%. ARK Challenge at 96. 9%. and comes very close in other metrics.\"\\n\\nThe transcript also discusses the distillation of the LLAMA models, where the smaller 70B and 8B models were distilled from the 405B model and showed significant performance improvements.\\n\\nIn summary, the key LLM-related announcements from Meta across these transcripts are:\\n1. The release of the Segment Anything Model (SAM) in March 2023, which enables efficient segmentation of objects in images and videos.\\n2. The release of the LLAMA 3.1 model, a 405 billion parameter model that outperforms GPT-4 on several benchmarks.\\n3. The distillation of smaller LLAMA models (70B and 8B) from the larger 405B model, resulting in significant performance improvements.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=150675, output_tokens=648))\n",
      "Total cost: $0.0381595 for 151343 input tokens and 259 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_0154F6We2xCJSjiC6tgGWdRN', content=[TextBlock(text='According to the transcripts, the only other major announcement from Meta that\\'s not related to large language models (LLMs) is the release of the Segment Anything Model (SAM) version 2.\\n\\nFrom the \"ThursdAI - August 1st\" transcript:\\n\\nThe transcript mentions that on Monday, Meta announced the release of Segment Anything Model 2 (SAM2), which is an updated version of the original Segment Anything Model:\\n\\n> \"So yesterday Monday, Meta, surprised us and announced that they are releasing Segment Anything 2. This was, yeah, this was on Monday. Segment Anything 2 is their, real time. Promptable object segmentation for images and video model.\"\\n\\nThe guests, Piotr Skalski and Joseph Nelson, discuss how SAM2 builds upon the capabilities of the original SAM model, allowing for real-time segmentation of objects in videos with a single click, and how this can be useful for various applications like video editing and sports analytics.\\n\\nOther than the updates to the Segment Anything Model, the transcripts do not mention any other major non-LLM related announcements from Meta.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=151343, output_tokens=259))\n",
      "Total cost: $0.03830649999999999 for 151626 input tokens and 320 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_01AE34UhHK8FcMRyGKviVKfZ', content=[TextBlock(text='According to the transcript \"ThursdAI - August 1st\", the company that provides caching for their LLM requests is DeepSeq.\\n\\nThe transcript discusses how DeepSeq introduced \"content caching on disk\" for their API, which provides significant cost savings for users:\\n\\n> \"DeepSeq basically implemented this kind of automatically. And, when you hit the caching on their end, you basically get a 90 percent reduction in price. Cachemist costs you 14 cents, which is still ridiculously cheap.\"\\n\\nThe transcript explains that caching is a big deal for LLMs because it helps address the problem of growing context windows and repeated queries, which can lead to unnecessary computation and increased costs:\\n\\n> \"The problem with large language models, especially with like agentic and content caching, Content, constantly repeating queries.There\\'s a lot of like repetitive inputs, especially when like with the chat, every time you send the chat query, the model basically gets all the previous chats, as an agent loop runs the, the, the model gets all the previous chats in every, in every new request, basically the context window grows.\"\\n\\nBy caching the responses, DeepSeq is able to bypass this recomputation, leading to a 90% reduction in cost per request. The transcript notes that this approach helps make using long context windows more feasible and cost-effective, which is a significant innovation for the LLM ecosystem.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=151626, output_tokens=320))\n",
      "Total cost: $0.03835475 for 151969 input tokens and 290 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_01N5ghS3QX9LDRpPnR1C3bki', content=[TextBlock(text='According to the \"ThursdAI - August 1st\" transcript, the model that does very well on vision tasks and is around 8 billion parameters is called OpenBMB\\'s 8 billion parameter model.\\n\\nThe transcript discusses this model in the following way:\\n\\n> \"OpenBMB, dropping a new vision model, a 8 billion parameter, mini CPM version 2. 6. it\\'s a VLM that understands images, multi images and video. And because it\\'s an 8 billion parameter, it can run basically on the phone. They actually have videos of it running on an iPad. It\\'s incredible in OCR as well.\"\\n\\nThe transcript notes that this 8 billion parameter model from OpenBMB is able to outperform GPT-4 on certain vision benchmarks, such as OCR Bench:\\n\\n> \"it gets the highest score on OCR bench, based on what they posted. There\\'s also the. 852 on OCRBench. [...] it gets the highest score as well. And then MMMU, which is like a massive multi model, evaluation. This gets almost 50.\"\\n\\nSo in summary, the model being referred to is the 8 billion parameter model from OpenBMB, which demonstrates strong performance on various vision-related tasks and benchmarks.', type='text')], model='claude-3-haiku-20240307', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=151969, output_tokens=290))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_output'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'price'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.11478024999999999</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1079.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'cache_creation_input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'cache_read_input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">453726.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'number_of_follow_up_questions'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ClaudePriceScorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cost'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'$0.23'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Input Tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">907452</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Output Tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2158</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cached Created Input Tokens '</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cached Read Input Tokens '</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38.12457203865051</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'model_output'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'price'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.11478024999999999\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'output_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1079.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'cache_creation_input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'cache_read_input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m453726.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'number_of_follow_up_questions'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'ClaudePriceScorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'Total Cost'\u001b[0m: \u001b[32m'$0.23'\u001b[0m,\n",
       "        \u001b[32m'Total Input Tokens'\u001b[0m: \u001b[1;36m907452\u001b[0m,\n",
       "        \u001b[32m'Total Output Tokens'\u001b[0m: \u001b[1;36m2158\u001b[0m,\n",
       "        \u001b[32m'Total Cached Created Input Tokens '\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "        \u001b[32m'Total Cached Read Input Tokens '\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m38.12457203865051\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.5715840000000001 for 63 input tokens and 440 output tokens with 150612 cache creation input tokens and 0 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_01CVKeJB4TKkgwh7sCa5A2jq', content=[TextBlock(text='Certainly! I\\'ll summarize the major LLM-related announcements from Meta across the transcripts in chronological order. I\\'ll include quotes from speakers and provide a brief summary for each announcement. Please note that my knowledge is based solely on the transcripts provided.\\n\\n1. July 25, 2023 - Meta Llama 3.1 Release (ThursdAI - July 25th.md)\\n\\nMeta released Llama 3.1, including three new models: 405B, 70B, and 8B parameters.\\n\\n> \"Meta released, and gave us three new models in open weights, including an incredibly detailed paper, just an incredibly detailed paper full of, full of nuggets, full of information.\"\\n\\nAlex Volkov summarizes the release, highlighting that the 405B parameter model beats GPT-4 on multiple benchmarks and has a 128K context window.\\n\\n2. August 1, 2023 - Segment Anything 2 (SAM2) Release (ThursdAI - August 1st.md)\\n\\nMeta released Segment Anything 2, a real-time promptable object segmentation model for images and video.\\n\\n> \"Segment Anything 2 comes out. Of course, it has all the previous capabilities of Segment Anything, so it still can work. do awesome things with images, faster, smaller, all the typical, updates coming with new releases of the models. But the segment anything tool is awesome because it also can segment objects on the video.\"\\n\\nPiotr Skalski explains that SAM2 can now work on videos, allowing users to annotate objects in one frame and have the segmentation propagate through the entire video.\\n\\nThese are the major LLM-related announcements from Meta that I could find in the provided transcripts. The Llama 3.1 release was a significant update to their language model lineup, while Segment Anything 2 represented an advancement in their computer vision capabilities.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=150612, cache_read_input_tokens=0, input_tokens=63, output_tokens=440))\n",
      "Total cost: $0.0537216 for 521 input tokens and 465 output tokens with 0 cache creation input tokens and 150612 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_018Rg5CQc4aQFySQkiBqbWzT', content=[TextBlock(text='Based on the transcripts provided, Meta made another significant announcement that\\'s not directly related to LLMs. This announcement was about their vision models. Here\\'s the details:\\n\\nFrom the transcript \"ThursdAI - August 1st.md\", Meta announced:\\n\\nSegment Anything 2 (SAM2)\\n\\nThis is a major update to their computer vision model, specifically for object segmentation in images and videos. While not an LLM, it\\'s a significant AI model release. Here are some key points about SAM2:\\n\\n1. Real-time video segmentation:\\n\\n> \"So now what you can do is you can have a video and only pick one frame of the video, annotate objects, that are interesting to you and you do that with like single point, which is awesome. So instead of going through the tedious process of drawing all the masks, you just click on an object and it immediately knows that you probably are interested in the whole person. And then what happens is that segmentation propagates through the whole video.\"\\n\\nPiotr Skalski explains that SAM2 can now work on videos, allowing users to annotate objects in one frame and have the segmentation propagate through the entire video.\\n\\n2. Open source and versatile:\\n\\n> \"It was released with Apache 2 license. it\\'s able to, with one click, detect, Objects across scenes\"\\n\\nAlex Volkov mentions that SAM2 is open source and can detect objects with a single click.\\n\\n3. Potential applications:\\n\\n> \"being able to,individually see and segment proteins to do,quick counting is a meaningful acceleration to seeing, how given cells respond to treatments,which, we\\'ve seen folks use to accelerate everything from,cancer research to,longevity\"\\n\\nJoseph Nelson explains potential uses of SAM2 in scientific research, demonstrating its versatility beyond just image and video editing.\\n\\nIn summary, while not an LLM, Segment Anything 2 represents a significant advancement in Meta\\'s AI capabilities, specifically in the field of computer vision and object segmentation. This model has potential applications ranging from video editing to scientific research.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=150612, input_tokens=521, output_tokens=465))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.0515976 for 63 input tokens and 415 output tokens with 0 cache creation input tokens and 150612 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_01V8RbTJkdVB2pbNpdnnjwYe', content=[TextBlock(text='Certainly! I\\'ll summarize the major LLM-related announcements from Meta across the transcripts in chronological order. I\\'ll include quotes from speakers and summarize each announcement briefly. My knowledge is based solely on the transcripts provided, so I\\'ll focus on the information available in those.\\n\\n1. Meta Llama 3.1 (July 25th, 2023)\\n\\nIn the transcript \"ThursdAI - July 25th.md\", Meta announced the release of Llama 3.1, including three new models: 8B, 70B, and 405B parameters.\\n\\n> \"Meta released, and gave us three new models in open weights, including an incredibly detailed paper, just an incredibly detailed paper full of, full of nuggets, full of information.\"\\n\\nThe 405B parameter model showed impressive performance, beating GPT-4 on multiple benchmarks. Meta also updated the license to allow for synthetic data creation and distillation.\\n\\n2. Segment Anything 2 (SAM2) (August 1st, 2023)\\n\\nIn the transcript \"ThursdAI - August 1st.md\", Meta announced the release of Segment Anything 2 (SAM2), a real-time promptable object segmentation model for images and video.\\n\\n> \"Segment Anything 2 comes out. Of course, it has all the previous capabilities of Segment Anything, so it still can work. do awesome things with images, faster, smaller, all the typical, updates coming with new releases of the models. But the segment anything tool is awesome because it also can segment objects on the video.\"\\n\\nSAM2 improved upon its predecessor by adding video segmentation capabilities and maintaining high performance with smaller model sizes.\\n\\nThese announcements demonstrate Meta\\'s continued commitment to advancing open-source AI models and technologies, particularly in the areas of large language models and computer vision.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=150612, input_tokens=63, output_tokens=415))\n",
      "Total cost: $0.053304599999999994 for 497 input tokens and 442 output tokens with 0 cache creation input tokens and 150612 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_017L6qoQAigApUNo4vi18JvJ', content=[TextBlock(text='Based on the transcripts provided, Meta also made a significant announcement in the field of computer vision that\\'s not directly related to LLMs. Here\\'s the details:\\n\\nMeta announced Segment Anything 2 (SAM2) on August 1st, 2023, as mentioned in the \"ThursdAI - August 1st.md\" transcript. While this isn\\'t an LLM, it\\'s a significant advancement in computer vision technology. Here are some key points about this announcement:\\n\\n1. SAM2 is an improvement over the original Segment Anything Model:\\n\\n> \"Segment Anything 2 comes out. Of course, it has all the previous capabilities of Segment Anything, so it still can work. do awesome things with images, faster, smaller, all the typical, updates coming with new releases of the models. But the segment anything tool is awesome because it also can segment objects on the video.\"\\n\\n2. It adds video segmentation capabilities:\\n\\n> \"So now what you can do is you have a video and only pick one frame of the video, annotate objects, that are interesting to you and you do that with like single point, which is awesome.\"\\n\\n3. It maintains object tracking even when objects leave and re-enter the frame:\\n\\n> \"And it doesn\\'t matter if the object goes out of the frame and comes back, it still knows that it\\'s the same object.\"\\n\\n4. The model comes in different sizes, with the smallest being quite efficient:\\n\\n> \"they have like four different sizes of the model, so of course you can expect different level of accuracy depending on the size. but the smallest one is like 40 million, the largest is like 250 million.\"\\n\\n5. It was released with an Apache 2 license, making it open source.\\n\\nThis announcement demonstrates Meta\\'s continued innovation in computer vision technologies, particularly in the area of object segmentation and tracking. SAM2 represents a significant advancement in real-time video analysis capabilities, which could have wide-ranging applications in various fields.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=150612, input_tokens=497, output_tokens=442))\n",
      "Total cost: $0.0572286 for 960 input tokens and 611 output tokens with 0 cache creation input tokens and 150612 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_013M7MGvtLZMHgXpbA8yvUHZ', content=[TextBlock(text='Based on the transcripts provided, the main discussion about caching for LLM requests comes from the \"ThursdAI - July 11.md\" transcript, where DeepSeq\\'s implementation of content caching is discussed. Here\\'s a summary of the information:\\n\\n1. DeepSeq\\'s Content Caching:\\n\\nDeepSeq introduced content caching on disk for their API. Alex Volkov mentions:\\n\\n> \"DeepSeq. The folks from DeepSec in their API introduced something called content caching on disk, which significantly, if, if you don\\'t know what content caching means, and we\\'ve talked about content caching after Google I. O. for a little bit, the problem with large language models, especially with like agentic and content caching, Content, constantly repeating queries.\"\\n\\n2. Why caching is a big deal:\\n\\na) Cost reduction:\\n> \"When you hit the caching on their end, you basically get a 90 percent reduction in price. Cachemist costs you 14 cents, which is still ridiculously cheap.\"\\n\\nb) Efficiency in handling repetitive inputs:\\n> \"There\\'s a lot of like repetitive inputs, especially when like with the chat, every time you send the chat query, the model basically gets all the previous chats, as an agent loop runs the, the, the model gets all the previous chats in every, in every new request, basically the context window grows.\"\\n\\nc) Improved performance for long-context applications:\\nNisten mentions:\\n> \"you need to load all the frames at once into the memory, so the amount of frames that you can process at once is a little bit of the limitating factor.\"\\n\\n3. Other companies implementing caching:\\n\\nWhile DeepSeq is the main focus of the discussion, there are mentions of other companies implementing similar features:\\n\\n- Google: \\n> \"the first time I heard about something like caching was, like productionized caching was from, from Google. They said that they\\'re going to introduce this in lower costs, but you have to opt into this.\"\\n\\n- Anthropic:\\n> \"cloud projects, if you use Sonnet 3. 5 and you use projects, is also part of context caching.\"\\n\\nThe importance of caching is emphasized throughout the discussion, with Alex Volkov stating:\\n\\n> \"I can\\'t believe I\\'m saying this. Like literally, I can\\'t believe that these words are coming out of my mouth. It costs one cent per million input tokens to do in this caching. if they\\'re cached, then, then, then it\\'s it\\'s basically nothing. Like it\\'s basically, you don\\'t even count this.\"\\n\\nThis demonstrates how significant the cost savings and performance improvements can be with effective caching implementations.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=150612, input_tokens=960, output_tokens=611))\n",
      "Total cost: $0.055248599999999995 for 1595 input tokens and 352 output tokens with 0 cache creation input tokens and 150612 cache read input tokens\n",
      "PromptCachingBetaMessage(id='msg_01UUyLBWbkfDRfg5YPUzJEyE', content=[TextBlock(text='Based on the information provided in the transcripts, the model you\\'re referring to is likely the CPM-Bee model from OpenBMB. This model was discussed in the \"ThursdAI - August 1st.md\" transcript. Here are the relevant details:\\n\\n1. The model is called CPM-Bee (or Mini CPM) version 2.6:\\n\\n> \"OpenBMB, 8 billion parameter, incredible vision model that\\'s probably state of the art, beats GPT 4 vision.\"\\n\\n2. It\\'s an 8 billion parameter model with impressive capabilities:\\n\\n> \"8 billion parameter like quantizes into a very small model.\"\\n\\n3. It performs exceptionally well on vision tasks, including OCR:\\n\\n> \"At OCR, it, there\\'s an benchmark. They\\'re called OCR bench. This model gets 852 points where G PT four oh gets 736. this model is better at OCR than Gemini. 1.5 than, than CLOs on it, at least on this benchmark.\"\\n\\n4. It can run on mobile devices:\\n\\n> \"And because it\\'s an 8 billion parameter, it can run basically on the phone. They actually have videos of it running on an iPad.\"\\n\\n5. It demonstrates strong performance across various tasks:\\n\\n> \"I\\'ve tried a few videos on my own and it was quite, quite stunning.\"\\n\\nThis model represents a significant advancement in compact, efficient vision models that can perform at a high level across various tasks, including OCR, image understanding, and video analysis, while being small enough to run on mobile devices.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=PromptCachingBetaUsage(cache_creation_input_tokens=0, cache_read_input_tokens=150612, input_tokens=1595, output_tokens=352))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_output'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'price'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.42134250000000006</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1362.5</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'cache_creation_input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75306.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'cache_read_input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">376530.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1849.5</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'number_of_follow_up_questions'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ClaudePriceScorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cost'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'$0.84'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Input Tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3699</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Output Tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2725</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cached Created Input Tokens '</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">150612</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cached Read Input Tokens '</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">753060</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">59.076401591300964</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'model_output'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'price'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.42134250000000006\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'output_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1362.5\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'cache_creation_input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m75306.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'cache_read_input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m376530.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1849.5\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'number_of_follow_up_questions'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'ClaudePriceScorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'Total Cost'\u001b[0m: \u001b[32m'$0.84'\u001b[0m,\n",
       "        \u001b[32m'Total Input Tokens'\u001b[0m: \u001b[1;36m3699\u001b[0m,\n",
       "        \u001b[32m'Total Output Tokens'\u001b[0m: \u001b[1;36m2725\u001b[0m,\n",
       "        \u001b[32m'Total Cached Created Input Tokens '\u001b[0m: \u001b[1;36m150612\u001b[0m,\n",
       "        \u001b[32m'Total Cached Read Input Tokens '\u001b[0m: \u001b[1;36m753060\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m59.076401591300964\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.459255 for 150675 input tokens and 482 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_01SWdc7wRYTgUJR2kdRccVFM', content=[TextBlock(text='Certainly! I\\'ll summarize the major LLM-related announcements from Meta across the transcripts in chronological order. I\\'ll include quotes from speakers and provide a brief summary for each announcement. Please note that my knowledge is based solely on the transcripts provided.\\n\\n1. Meta LLAMA 3.1 (July 25th, 2023)\\nFrom \"ThursdAI - July 25th.md\":\\n\\n> \"Meta released, and gave us three new models in open weights, including an incredibly detailed paper, just an incredibly detailed paper full of, full of nuggets, full of information.\"\\n\\nMeta released LLAMA 3.1 with three new models: 405 billion parameters, 70 billion parameters, and 8 billion parameters. The 405B model beats GPT-4 on multiple benchmarks and has a 128K context window.\\n\\n2. Meta Segment Anything 2 (SAM2) (August 1st, 2023)\\nFrom \"ThursdAI - August 1st.md\":\\n\\n> \"Segment Anything 2 is their, real time. Promptable object segmentation for images and video model. segment anything before this was like a very important model before.\"\\n\\nMeta released Segment Anything 2, an improved version of their object segmentation model for images and video. It\\'s faster, smaller, and can now segment objects in videos with just a single click.\\n\\n3. Meta Llama 3.5 V (Mentioned but not released) (August 1st, 2023)\\nFrom \"ThursdAI - August 1st.md\":\\n\\n> \"Lama 3. 5 V. is a thing. And in the paper, they actually talk about that they\\'ve trained, but not released a vision model and multimodal model of Lama 3. 1.\"\\n\\nMeta mentioned training Llama 3.5 V, a multimodal vision model, but didn\\'t release it due to European AI regulations. They provided insights into its training process in the paper.\\n\\nThese announcements showcase Meta\\'s continued efforts in advancing open-source AI models, particularly in language modeling and computer vision tasks.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=150675, output_tokens=482))\n",
      "Total cost: $0.459867 for 151179 input tokens and 422 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_018cu8cxbjWWXYX2MSXh1V3h', content=[TextBlock(text='Based on the transcripts provided, Meta made an additional announcement not directly related to LLMs:\\n\\nMeta released Segment Anything 2 (SAM2) on August 1st, 2023. This announcement is related to computer vision rather than language models. From the \"ThursdAI - August 1st.md\" transcript:\\n\\n> \"Meta released Segment Anything 2. This was, yeah, this was on Monday. Segment Anything 2 is their, real time. Promptable object segmentation for images and video model.\"\\n\\nPiotr Skalski from Roboflow provided more details about SAM2:\\n\\n> \"So now, Segment Anything 2 comes out. Of course, it has all the previous capabilities of Segment Anything, so it still can work. do awesome things with images, faster, smaller, all the typical, updates coming with new releases of the models. But the segment anything tool is awesome because it also can segment objects on the video.\"\\n\\nSAM2 is an improvement over the original Segment Anything model, with the following key features:\\n\\n1. It can now segment objects in videos, not just images.\\n2. It allows for one-click annotation that propagates through the entire video.\\n3. It can track objects even if they leave and re-enter the frame.\\n4. The model is faster and smaller than its predecessor.\\n5. It was released with an Apache 2 license, making it open-source.\\n\\nJoseph Nelson from Roboflow highlighted some potential applications:\\n\\n> \"being able to, individually see and segment proteins to do, quick counting is a meaningful acceleration to seeing, how given cells respond to treatments, which, we\\'ve seen folks use to accelerate everything from, cancer research to, longevity\"\\n\\nThis announcement demonstrates Meta\\'s continued commitment to advancing open-source AI technologies in the field of computer vision, alongside their work on language models.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=151179, output_tokens=422))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m1\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cost: $0.46245 for 150675 input tokens and 695 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_01YGCHpsw2ZZ97BkVEyXm4He', content=[TextBlock(text='Certainly! I\\'ll summarize the major LLM-related announcements from Meta across the transcripts in chronological order. I\\'ll include quotes from speakers and provide a brief summary for each announcement. Please note that my knowledge is based solely on the transcripts provided.\\n\\n1. Meta Llama 3 (April 2023)\\nTranscript: ThursdAI - July 25th.md\\n\\n> \"Yeah, I wanted to say, yes, the original release was actually a pre release as Mark said, because, he wanted to put out the 3 before it was really ready, so that was the thing, and I was pretty disappointed with 3 because of the small context, only 8k, I mean it is what, was double what we had before.\" - Wolfram Ravenwolf\\n\\nMeta released Llama 3 with an 8k context window, which was an improvement but still disappointing to some users due to its limited context size compared to other models.\\n\\n2. Meta Llama 3.1 (July 2023)\\nTranscript: ThursdAI - July 25th.md\\n\\n> \"Meta released, and gave us three new models in open weights, including an incredibly detailed paper, just an incredibly detailed paper full of, full of nuggets, full of information.\" - Alex Volkov\\n\\nMeta released Llama 3.1, which included three new models: 405 billion parameters, 70 billion parameters, and 8 billion parameters. The release came with a detailed paper and significant improvements in performance.\\n\\n> \"405 billion parameters is finally here. The biggest, Metal Llama 3. 1, 405 billion parameters. It\\'s a dense model. 4 level intelligence. In fact, it beats GPT 4. 0 on multiple benchmarks.\" - Alex Volkov\\n\\nThe 405B model showed GPT-4 level performance on multiple benchmarks, marking a significant achievement for open-source models.\\n\\n3. Meta Segment Anything 2 (SAM2) (July 31, 2023)\\nTranscript: ThursdAI - August 1st.md\\n\\n> \"Meta, surprised us and announced that they are releasing Segment Anything 2. This was, yeah, this was on Monday. Segment Anything 2 is their, real time. Promptable object segmentation for images and video model.\" - Alex Volkov\\n\\nMeta released Segment Anything 2, an advanced model for real-time object segmentation in images and videos. This model improved upon its predecessor with video capabilities and faster processing.\\n\\n> \"So instead of going through the tedious process of drawing all the masks, you just click on an object and it immediately knows that you probably are interested in the whole person. And then what happens is that segmentation propagates through the whole video.\" - Piotr Skalski\\n\\nSAM2 introduced significant improvements in ease of use and efficiency for object segmentation tasks, especially in video processing.\\n\\nThese announcements showcase Meta\\'s continued commitment to advancing open-source AI models and tools, with each release bringing significant improvements in performance, capabilities, and ease of use.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=150675, output_tokens=695))\n",
      "Total cost: $0.461208 for 151391 input tokens and 469 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_01KFvUgYkUFkakb1yGyBayvR', content=[TextBlock(text='Based on the transcripts provided, Meta also made a significant announcement not directly related to LLMs. Here\\'s the details:\\n\\nMeta announced Segment Anything 2 (SAM2) on July 31, 2023. This is related to computer vision rather than language models. Here are the key points from the transcript \"ThursdAI - August 1st.md\":\\n\\n> \"Meta, surprised us and announced that they are releasing Segment Anything 2. This was, yeah, this was on Monday. Segment Anything 2 is their, real time. Promptable object segmentation for images and video model.\" - Alex Volkov\\n\\nSAM2 is an advancement in object segmentation for both images and videos. It\\'s a significant improvement over the original Segment Anything model. \\n\\nPiotr Skalski provided more details about its capabilities:\\n\\n> \"So instead of going through the tedious process of drawing all the masks, you just click on an object and it immediately knows that you probably are interested in the whole person. And then what happens is that segmentation propagates through the whole video. So, you only annotate, one or two frames, and then get all the other frames for free. And it doesn\\'t matter if the object goes out of the frame and comes back, it still knows that it\\'s the same\"\\n\\nThis model simplifies the process of object segmentation in videos, making it much more user-friendly and efficient. It can track objects across frames, even when they temporarily leave the frame.\\n\\nJoseph Nelson from Roboflow highlighted its potential applications:\\n\\n> \"being able to, individually see and segment proteins to do,quick counting is a meaningful acceleration to seeing, how given cells respond to treatments,which, we\\'ve seen folks use to accelerate everything from,cancer research to,longevity\"\\n\\nSAM2 has potential applications in various fields, from sports broadcasting to scientific research, including cell biology and cancer research.\\n\\nThe model was released with an Apache 2 license, making it openly available for use and further development by the community. This announcement represents Meta\\'s continued investment in open-source AI tools beyond just language models.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=151391, output_tokens=469))\n",
      "Total cost: $0.46646400000000005 for 151883 input tokens and 721 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_012VERStqcwCZhCuTPkVbtrJ', content=[TextBlock(text='Based on the transcripts provided, the main discussion about caching for LLM requests is focused on DeepSeq. Here\\'s a summary of the information from the transcript \"ThursdAI - August 1st.md\":\\n\\nDeepSeq introduced content caching on disk for their API. This is considered a significant development in the field of LLM API providers. Here\\'s why caching is a big deal:\\n\\n1. Cost Reduction:\\nAlex Volkov explains:\\n\\n> \"When you hit the caching on their end, you basically get a 90 percent reduction in price. Cachemist costs you 14 cents, which is still ridiculously cheap. I think like 14 cents is one of the cheaper APIs out there, if I\\'m not mistaken, for this level of intelligence. 14 cents per input tokens, per, M tokens per million tokens. If you hit the cash, it\\'s going to be 0. 14. So there\\'s going to be like one, 1. 4 cents per million tokens, if you hit cash\"\\n\\nThis significant price reduction makes repeated queries much more cost-effective.\\n\\n2. Efficiency:\\nCaching helps avoid unnecessary recomputation, especially in scenarios with repetitive inputs or context. Alex mentions:\\n\\n> \"Content, constantly repeating queries. There\\'s a lot of like repetitive inputs, especially when like with the chat, every time you send the chat query, the model basically gets all the previous chats, as an agent loop runs the, the, the model gets all the previous chats in every, in every new request, basically the context window grows.\"\\n\\n3. Performance in Long-Context Scenarios:\\nMatt Shumer points out:\\n\\n> \"I think there are definitely other. Really interesting things like, put your entire code base in there, that sort of stuff. But for me and how we use LLMs in our products, I am very, very excited about the idea of like,instead of generating, or instead of gathering a million examples and doing that fine tuning process and going through the hell that that is, because it\\'s not fun, instead just saying, okay, let\\'s curate a hundred gold examples that we know are fantastic and show the model the few things we really wanted to do really well, and suddenly you have something that\\'s better than if you fine tuned it, and it\\'s cheaper, and it\\'s faster, and you don\\'t have to worry about managing infra.\"\\n\\nThis suggests that caching can be particularly beneficial for scenarios requiring long context or repeated use of specific examples.\\n\\n4. Potential Industry Standard:\\nAlex Volkov suggests this might become an industry standard:\\n\\n> \"I\\'m hoping that more folks will implement this and Nisten, you\\'re probably right. Like it\\'s implemented behind the scenes for many folks. Like I\\'m hoping that more folks in the API provider layer will expose this to developers.\"\\n\\nWhile DeepSeq is the main focus of the discussion, it\\'s mentioned that other providers might be implementing similar features behind the scenes. For example, Claude from Anthropic is mentioned as potentially using a form of caching in their \\'Projects\\' feature.\\n\\nThe transcripts don\\'t provide comprehensive information about which other companies are providing caching, but they do emphasize the potential importance of this feature in the LLM API landscape going forward.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=151883, output_tokens=721))\n",
      "Total cost: $0.463908 for 152631 input tokens and 401 output tokens with 0 cache creation input tokens and 0 cache read input tokens\n",
      "Message(id='msg_016vxxKTqsGhSisP4mFiLdH6', content=[TextBlock(text='Based on the transcripts provided, the model you\\'re referring to is likely the OpenBMB\\'s Mini CPM version 2.6. This information comes from the transcript \"ThursdAI - August 1st.md\". Here are the relevant details:\\n\\n> \"We also talked about OpenBMB, 8 billion parameter, incredible vision model that\\'s probably state of the art, beats GPT 4 vision. It\\'s quite incredible to me that we keep saying open source beats GPT 4, open source beats GPT 4, and it\\'s not even a, like a crazy thing to us.\" - Alex Volkov\\n\\nAlex Volkov further describes the model\\'s capabilities:\\n\\n> \"It\\'s incredible in OCR as well. And I, I, I\\'ve read out a few examples of videos that I, I uploaded and it captioned. And it\\'s quite incredible. It\\'s incredible because again, we\\'re getting a GPT 4 level vision model. 8 billion parameters.\"\\n\\nThe model shows impressive performance on various vision tasks:\\n\\n> \"At OCR, it, there\\'s an benchmark. They\\'re called OCR bench. This model gets 852 points where G PT four oh gets 736. this model is better at OCR than Gemini. 1.5 than, than CLOs on it, at least on this benchmark.\"\\n\\nAdditionally, the model can run on mobile devices:\\n\\n> \"And because it\\'s an 8 billion parameter, it can run basically on the phone. They actually have videos of it running on an iPad.\"\\n\\nSo, to summarize, the model you\\'re asking about is the OpenBMB Mini CPM version 2.6. It\\'s an 8 billion parameter vision model that shows impressive performance on various vision tasks, including OCR, and can run on mobile devices.', type='text')], model='claude-3-5-sonnet-20240620', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=Usage(input_tokens=152631, output_tokens=401))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluated <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> examples\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluated \u001b[1;36m2\u001b[0m of \u001b[1;36m2\u001b[0m examples\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluation summary\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_output'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'price'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.386576</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'output_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1595.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'cache_creation_input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'cache_read_input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'input_tokens'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">454217.0</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'number_of_follow_up_questions'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'ClaudePriceScorer'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cost'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'$2.77'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Input Tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">908434</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Output Tokens'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3190</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cached Created Input Tokens '</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Total Cached Read Input Tokens '</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'model_latency'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67.35225999355316</span><span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Evaluation summary\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'model_output'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'price'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1.386576\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'output_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m1595.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'cache_creation_input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'cache_read_input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m0.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'input_tokens'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m454217.0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[32m'number_of_follow_up_questions'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'ClaudePriceScorer'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'Total Cost'\u001b[0m: \u001b[32m'$2.77'\u001b[0m,\n",
       "        \u001b[32m'Total Input Tokens'\u001b[0m: \u001b[1;36m908434\u001b[0m,\n",
       "        \u001b[32m'Total Output Tokens'\u001b[0m: \u001b[1;36m3190\u001b[0m,\n",
       "        \u001b[32m'Total Cached Created Input Tokens '\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "        \u001b[32m'Total Cached Read Input Tokens '\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'model_latency'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'mean'\u001b[0m: \u001b[1;36m67.35225999355316\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# what I want: 4 functions, 1 that calls Haiku without cache, 1 that calls Haiku with cache, 1 that calls Sonnet without cache, 1 that calls Sonnet with cache, each function is wrapped with weave.op and each function is calling \n",
    "from time import sleep\n",
    "\n",
    "@weave.op\n",
    "def claude_requests_wrapper(number_of_questions: int, model=FAST_MODEL, cached=False) -> dict:\n",
    "    conversation_history = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    total_price = 0\n",
    "    total_cache_creation_input_tokens = 0\n",
    "    total_cache_read_input_tokens = 0\n",
    "    \n",
    "    for i in range(number_of_questions):\n",
    "        user_message = my_transcript_questions[i][\"question\"]\n",
    "        if cached:\n",
    "            model_response =  get_claude_response_with_caching(conversation_history, user_message, model=model)\n",
    "        else:\n",
    "            model_response =  get_claude_response(conversation_history, user_message, model=model)\n",
    "        conversation_history = model_response.get('conversation_history', [])\n",
    "\n",
    "        total_input_tokens += model_response[\"price\"][\"input_tokens\"]\n",
    "        total_output_tokens += model_response[\"price\"][\"output_tokens\"]\n",
    "        total_price += model_response[\"price\"][\"total_cost\"]\n",
    "        total_cache_creation_input_tokens += model_response[\"price\"][\"cache_creation_input_tokens\"]\n",
    "        total_cache_read_input_tokens += model_response[\"price\"][\"cache_read_input_tokens\"]\n",
    "    \n",
    "    return {\n",
    "        \"number_of_follow_up_questions\": number_of_questions,\n",
    "        \"response_text\": model_response.get('response_text', ''),\n",
    "        \"input_tokens\": total_input_tokens,\n",
    "        \"output_tokens\": total_output_tokens,\n",
    "        \"cache_creation_input_tokens\": total_cache_creation_input_tokens,\n",
    "        \"cache_read_input_tokens\": total_cache_read_input_tokens,\n",
    "        \"price\": total_price\n",
    "    }\n",
    "\n",
    "@weave.op\n",
    "def haiku_cached(number_of_questions: int) -> dict:\n",
    "    print(f\"Running {number_of_questions} questions with Haiku cached\")\n",
    "    return claude_requests_wrapper(number_of_questions, model=FAST_MODEL, cached=True)\n",
    "\n",
    "@weave.op\n",
    "def haiku_uncached(number_of_questions: int) -> dict:\n",
    "    return claude_requests_wrapper(number_of_questions, model=FAST_MODEL, cached=False)\n",
    "\n",
    "@weave.op\n",
    "def sonnet_cached(number_of_questions: int) -> dict:\n",
    "    return claude_requests_wrapper(number_of_questions, model=SMART_MODEL, cached=True)\n",
    "\n",
    "@weave.op\n",
    "def sonnet_uncached(number_of_questions: int) -> dict:\n",
    "    return claude_requests_wrapper(number_of_questions, model=SMART_MODEL, cached=False)\n",
    " \n",
    "\n",
    "haiku_uncached_eval = weave.Evaluation(dataset=evaluation_dataset, scorers=[ClaudePriceScorer()], name=\"Uncached Haiku\")\n",
    "haiku_cached_eval = weave.Evaluation(dataset=evaluation_dataset, scorers=[ClaudePriceScorer()], name=\"Cached Haiku\")\n",
    "sonnet_cached_eval = weave.Evaluation(dataset=evaluation_dataset, scorers=[ClaudePriceScorer()], name=\"Cached Sonnet\")\n",
    "sonnet_uncached_eval = weave.Evaluation(dataset=evaluation_dataset, scorers=[ClaudePriceScorer()], name=\"Uncached Sonnet\")\n",
    "\n",
    "\n",
    "claude_haiku_cached_eval_results = await haiku_cached_eval.evaluate(haiku_cached)\n",
    "sleep(120)\n",
    "claude_haiku_uncached_eval_results = await haiku_uncached_eval.evaluate(haiku_uncached)\n",
    "sleep(120)\n",
    "claude_sonnet_cached_eval_results = await sonnet_cached_eval.evaluate(sonnet_cached)\n",
    "sleep(120)\n",
    "claude_sonnet_uncached_eval_results = await sonnet_uncached_eval.evaluate(sonnet_uncached)\n",
    "sleep(120)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comparellamas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
